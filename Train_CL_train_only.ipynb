{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train CL - Train Data Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dependencies & Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from py_irt import scoring\n",
    "from torchinfo import summary\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sortedcontainers import SortedDict\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\aug_data\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\train_only_data\\task\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\train_only_data\\valid_task\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\train_only_data\\fitted_IRT\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'aug_data')\n",
    "task_path = os.path.join(cwd, 'train_only_data', 'task')\n",
    "valid_task_path = os.path.join(cwd, 'train_only_data', 'valid_task')\n",
    "difficulties_path = os.path.join(cwd, 'train_only_data', 'fitted_IRT')\n",
    "\n",
    "\n",
    "print(cwd)\n",
    "print(data_path)\n",
    "print(task_path)\n",
    "print(valid_task_path)\n",
    "print(difficulties_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a852dcf430>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4747\n",
    "\n",
    "# numpy\n",
    "np.random.seed(seed)\n",
    "# random\n",
    "random.seed(seed)\n",
    "# pytorch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ability', 'diff', 'irt_model', 'item_ids', 'subject_ids'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class = \"Perspective\"\n",
    "\n",
    "file_va = json.load(open(os.path.join(difficulties_path, train_class, 'best_parameters.json')))\n",
    "file_va.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.2691941261291504,\n",
       " -2.416630744934082,\n",
       " -2.4665095806121826,\n",
       " -2.4016575813293457,\n",
       " -1.2968376874923706]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_va['diff'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '7db0d3a1-b8e4-4ff6-96e3-cc55d9cb77a3'),\n",
       " ('1', '2b8e660a-ebfc-4c18-a3d3-295fed1dc8f4'),\n",
       " ('2', '6317c474-d3fd-418d-9fa0-217506a75f56'),\n",
       " ('3', '510ff0b4-8773-41d1-9182-e1078f6336b4'),\n",
       " ('4', 'd5eafd31-3a7c-4746-9329-1639ee8aee14')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(file_va['item_ids'].items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86b8c8eb-0df9-4ebc-b9cc-5c284e449cc7</td>\n",
       "      <td>-1.269194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bf1b327c-f966-4d39-b154-ebc1015f9ed7</td>\n",
       "      <td>-2.416631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>703c5529-bd01-436a-8574-9582785cb1e1</td>\n",
       "      <td>-2.466510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ec5d70c-8668-407c-bf36-8d78bed88995</td>\n",
       "      <td>-2.401658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1fcde17b-18c4-49ac-90ed-1310228cae9d</td>\n",
       "      <td>-1.296838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID      diff\n",
       "0  86b8c8eb-0df9-4ebc-b9cc-5c284e449cc7 -1.269194\n",
       "1  bf1b327c-f966-4d39-b154-ebc1015f9ed7 -2.416631\n",
       "2  703c5529-bd01-436a-8574-9582785cb1e1 -2.466510\n",
       "3  5ec5d70c-8668-407c-bf36-8d78bed88995 -2.401658\n",
       "4  1fcde17b-18c4-49ac-90ed-1310228cae9d -1.296838"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [file_va['item_ids'][i] for i in file_va['item_ids']]\n",
    "diff_data = pd.DataFrame({'ID': ids, 'diff': file_va['diff']})\n",
    "diff_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Merge all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Country\n",
      "Train size: 34996\n",
      "Merging Intervention\n",
      "Train size: 34997\n",
      "Merging Perspective\n",
      "Train size: 34995\n",
      "Merging Population\n",
      "Train size: 34998\n",
      "Merging Sample Size\n",
      "Train size: 34993\n",
      "Merging Study Period\n",
      "Train size: 34997\n"
     ]
    }
   ],
   "source": [
    "labels = ['Country','Intervention','Perspective','Population','Sample Size','Study Period']\n",
    "IRT_train = {}\n",
    "\n",
    "for train_class in labels:\n",
    "    print(f'Merging {train_class}')\n",
    "    # load task & diff\n",
    "    file_va = json.load(open(os.path.join(difficulties_path, train_class, 'best_parameters.json')))\n",
    "    ids = [file_va['item_ids'][i] for i in file_va['item_ids']]\n",
    "    diff_data = pd.DataFrame({'ID': ids, 'diff': file_va['diff']})\n",
    "    task_data = pd.read_csv(os.path.join(task_path, f'{train_class}.csv'))\n",
    "    merged_df = pd.merge(task_data, diff_data, on='ID')\n",
    "    merged_df['Pos_support_locs'] = merged_df['Pos_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    merged_df['Neg_support_locs'] = merged_df['Neg_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    IRT_train[train_class] = merged_df[[\"ID\",\"Pos_support_locs\",\"Neg_support_locs\", \"Query_loc\",\"Label\",\"Alpha\",\"Aug_type\",\"diff\"]]\n",
    "    print(f'Train size: {len(merged_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Country\n",
      "Train size: 14983\n",
      "Merging Intervention\n",
      "Train size: 14985\n",
      "Merging Perspective\n",
      "Train size: 14981\n",
      "Merging Population\n",
      "Train size: 14982\n",
      "Merging Sample Size\n",
      "Train size: 14980\n",
      "Merging Study Period\n",
      "Train size: 14988\n"
     ]
    }
   ],
   "source": [
    "labels = ['Country','Intervention','Perspective','Population','Sample Size','Study Period']\n",
    "IRT_eval = {}\n",
    "\n",
    "for train_class in labels:\n",
    "    print(f'Merging {train_class}')\n",
    "    # load task\n",
    "    task_data = pd.read_csv(os.path.join(valid_task_path, f'{train_class}.csv'))\n",
    "    task_data['Pos_support_locs'] = task_data['Pos_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    task_data['Neg_support_locs'] = task_data['Neg_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    IRT_eval[train_class] = task_data[[\"ID\",\"Pos_support_locs\",\"Neg_support_locs\", \"Query_loc\",\"Label\",\"Alpha\",\"Aug_type\"]]\n",
    "    print(f'Train size: {len(task_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005edd-1296-4630-9789-b5a5a23972f1</td>\n",
       "      <td>[6, 225, 181, 0, 211]</td>\n",
       "      <td>[596, 207, 117, 322, 249]</td>\n",
       "      <td>417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.371309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001c7fa-751f-478a-9e04-bfc836a0a514</td>\n",
       "      <td>[4, 66, 181, 179, 11]</td>\n",
       "      <td>[111, 177, 634, 508, 630]</td>\n",
       "      <td>292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.408183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00036e5c-f52d-4c1c-a88e-bae07525c43a</td>\n",
       "      <td>[34, 31, 35, 211, 58]</td>\n",
       "      <td>[443, 488, 583, 241, 268]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.038628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061dae-95a0-4afe-aee0-fb4baf6be08f</td>\n",
       "      <td>[225, 7, 164, 39, 33]</td>\n",
       "      <td>[83, 347, 186, 424, 314]</td>\n",
       "      <td>385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.268913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008324d-24a1-4489-922d-2ddff8a7519f</td>\n",
       "      <td>[5, 3, 67, 11, 51]</td>\n",
       "      <td>[449, 318, 506, 596, 503]</td>\n",
       "      <td>651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.429216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID       Pos_support_locs  \\\n",
       "0  00005edd-1296-4630-9789-b5a5a23972f1  [6, 225, 181, 0, 211]   \n",
       "1  0001c7fa-751f-478a-9e04-bfc836a0a514  [4, 66, 181, 179, 11]   \n",
       "2  00036e5c-f52d-4c1c-a88e-bae07525c43a  [34, 31, 35, 211, 58]   \n",
       "3  00061dae-95a0-4afe-aee0-fb4baf6be08f  [225, 7, 164, 39, 33]   \n",
       "4  0008324d-24a1-4489-922d-2ddff8a7519f     [5, 3, 67, 11, 51]   \n",
       "\n",
       "            Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0  [596, 207, 117, 322, 249]        417    0.0      1         3 -2.371309  \n",
       "1  [111, 177, 634, 508, 630]        292    0.0      3         1 -2.408183  \n",
       "2  [443, 488, 583, 241, 268]          0    1.0      5         4  1.038628  \n",
       "3   [83, 347, 186, 424, 314]        385    0.0      0         4  0.268913  \n",
       "4  [449, 318, 506, 596, 503]        651    0.0      4         1 -2.429216  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRT_train['Perspective'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002b905-1318-4a7a-b182-6bb79663e886</td>\n",
       "      <td>[162, 8, 204, 179, 22]</td>\n",
       "      <td>[306, 145, 342, 331, 490]</td>\n",
       "      <td>117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003fe41-aa46-4334-985c-d3e60ee43e73</td>\n",
       "      <td>[4, 36, 57, 5, 13]</td>\n",
       "      <td>[435, 121, 472, 190, 620]</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006f5e9-e00a-41f6-81f4-484cb153a0f5</td>\n",
       "      <td>[25, 15, 28, 35, 162]</td>\n",
       "      <td>[437, 126, 583, 628, 634]</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000bb31e-883e-4a48-9c70-752ad9f23d99</td>\n",
       "      <td>[11, 58, 187, 57, 28]</td>\n",
       "      <td>[564, 220, 333, 169, 277]</td>\n",
       "      <td>212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000bb35c-7762-4af1-bdb7-e7373ffecda1</td>\n",
       "      <td>[47, 68, 216, 4, 15]</td>\n",
       "      <td>[171, 421, 155, 447, 593]</td>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID        Pos_support_locs  \\\n",
       "0  0002b905-1318-4a7a-b182-6bb79663e886  [162, 8, 204, 179, 22]   \n",
       "1  0003fe41-aa46-4334-985c-d3e60ee43e73      [4, 36, 57, 5, 13]   \n",
       "2  0006f5e9-e00a-41f6-81f4-484cb153a0f5   [25, 15, 28, 35, 162]   \n",
       "3  000bb31e-883e-4a48-9c70-752ad9f23d99   [11, 58, 187, 57, 28]   \n",
       "4  000bb35c-7762-4af1-bdb7-e7373ffecda1    [47, 68, 216, 4, 15]   \n",
       "\n",
       "            Neg_support_locs  Query_loc  Label  Alpha  Aug_type  \n",
       "0  [306, 145, 342, 331, 490]        117    0.0      0         0  \n",
       "1  [435, 121, 472, 190, 620]         73    1.0      0         0  \n",
       "2  [437, 126, 583, 628, 634]         53    1.0      0         0  \n",
       "3  [564, 220, 333, 169, 277]        212    1.0      0         0  \n",
       "4  [171, 421, 155, 447, 593]        272    0.0      0         0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRT_eval['Perspective'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Load Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 660, 100, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_token_embeddings = np.load(os.path.join(data_path, 'wordvector_masked_aug.npy'))\n",
    "aug_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 100, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = np.load(os.path.join(data_path, 'train_wordvectors.npy'))\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, shuffle=self.shuffle).__iter__()\n",
    "\n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            return next(self.data_loader)\n",
    "        except StopIteration:\n",
    "            self.data_loader = DataLoader(self.dataset, self.batch_size, shuffle=self.shuffle).__iter__()\n",
    "            return self.next_batch()\n",
    "\n",
    "\n",
    "class CL_diff_generator(Dataset):\n",
    "    def __init__(self, wv, aug_wv, id_subset, extract_dict, valid):\n",
    "        self.wv = wv\n",
    "        self.aug_wv = aug_wv\n",
    "        self.extract_dict = extract_dict\n",
    "        self.id_subset = id_subset\n",
    "        self.valid = valid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cur_id = self.id_subset[index]\n",
    "        cur_support_data, cur_query_data_x, cur_query_data_y = self.get_one_element_of_batch(cur_query_dict=self.extract_dict[cur_id])\n",
    "        if not self.valid:\n",
    "            cur_diff = self.extract_dict[cur_id]['diff']\n",
    "            return cur_support_data, cur_query_data_x, cur_query_data_y, cur_diff\n",
    "        else:\n",
    "            return cur_support_data, cur_query_data_x, cur_query_data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_subset)\n",
    "\n",
    "    def get_one_element_of_batch(self, cur_query_dict):\n",
    "        # extract variables\n",
    "        cur_alpha = cur_query_dict['Alpha']\n",
    "        cur_aug_type = cur_query_dict['Aug_type']\n",
    "        try:\n",
    "            cur_pos_support_locs = ast.literal_eval(cur_query_dict['Pos_support_locs'])\n",
    "        except ValueError as e:\n",
    "            if isinstance(cur_query_dict['Pos_support_locs'], list):\n",
    "                cur_pos_support_locs = cur_query_dict['Pos_support_locs']\n",
    "            else:\n",
    "                raise e\n",
    "        try:\n",
    "            cur_neg_support_locs = ast.literal_eval(cur_query_dict['Neg_support_locs'])\n",
    "        except ValueError as e:\n",
    "            if isinstance(cur_query_dict['Neg_support_locs'], list):\n",
    "                cur_neg_support_locs = cur_query_dict['Neg_support_locs']\n",
    "            else:\n",
    "                raise e\n",
    "        cur_query_loc = int(cur_query_dict['Query_loc'])\n",
    "        cur_query_label = cur_query_dict['Label']\n",
    "\n",
    "        # support\n",
    "        if cur_alpha != 0:\n",
    "            support_pos = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_pos_support_locs]\n",
    "            support_neg = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_neg_support_locs]\n",
    "            # FIXME: support_data = np.concatenate((support_pos, support_neg), axis=0)  negative comes first\n",
    "            support_data = np.concatenate((support_neg, support_pos), axis=0)\n",
    "\n",
    "            # query\n",
    "            query_data = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_query_loc]\n",
    "        else:\n",
    "            support_pos = self.wv[cur_pos_support_locs]\n",
    "            support_neg = self.wv[cur_neg_support_locs]\n",
    "            # support_data = np.concatenate((support_pos, support_neg), axis=0) negative comes first\n",
    "            support_data = np.concatenate((support_neg, support_pos), axis=0)\n",
    "\n",
    "            # query\n",
    "            query_data = self.wv[cur_query_loc]\n",
    "\n",
    "        return support_data, query_data, int(cur_query_label)\n",
    "\n",
    "class CL_dff_loader:\n",
    "    def __init__(self, data_df, batch_size, wv, aug_wv, n_shot):\n",
    "        # attributes\n",
    "        self.wv = wv\n",
    "        self.aug_wv = aug_wv\n",
    "        self.n_shot = n_shot\n",
    "        self.batch_size = batch_size\n",
    "        # extract info from data_df & diff look up dict\n",
    "        self.extract_dict = {}\n",
    "        self.diff_dict = {}\n",
    "        if 'diff' in data_df.columns:\n",
    "            for _, cur_row in data_df.iterrows():\n",
    "                self.extract_dict[cur_row['ID']] = dict(cur_row)\n",
    "                self.diff_dict[cur_row['ID']] = cur_row['diff']\n",
    "        else:\n",
    "            for _, cur_row in data_df.iterrows():\n",
    "                self.extract_dict[cur_row['ID']] = dict(cur_row)\n",
    "                self.diff_dict[cur_row['ID']] = math.nan\n",
    "\n",
    "    def batch_generator_with_theta(self, theta):\n",
    "        # find (diff < theta) ids\n",
    "        id_subset = [cur_key for cur_key in self.diff_dict if self.diff_dict[cur_key] <= theta]\n",
    "\n",
    "        if not id_subset:\n",
    "            raise NoSample\n",
    "        elif len(id_subset) == 1:\n",
    "            raise OnlyOneSample\n",
    "\n",
    "        # construct dataloader\n",
    "        cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict, valid=False)\n",
    "        cur_dateset_len = len(cur_dataset)\n",
    "        return InfiniteDataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True), cur_dateset_len\n",
    "\n",
    "\n",
    "    def batch_generator_without_theta(self, subsample_size=-1, valid=False):  # if subsample_size == -1, use all samples & infinite dataloader | if subsample_size != -1, subsample & use finite dataloader\n",
    "        # whole dataset\n",
    "        id_subset = list(self.diff_dict)\n",
    "        if subsample_size == -1:\n",
    "            cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict, valid=valid)\n",
    "            return InfiniteDataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            id_subset = random.sample(id_subset, subsample_size)\n",
    "            # construct dataloader\n",
    "            cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict, valid=valid)\n",
    "            return DataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class NoSample(Exception):\n",
    "    def __init__(self):\n",
    "        self.message = \"No sample's diff is lower than current theta\"\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class OnlyOneSample(Exception):\n",
    "    def __init__(self):\n",
    "        self.message = \"Only one sample is found, minimum requirement: 2\"\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def data_converter(support_data, query_data_x, query_data_y, n_shot, predict_mode=False):\n",
    "    \"\"\"\n",
    "    convert the data to the fomat of Protonet class, only apply to two classes\n",
    "\n",
    "    Args:\n",
    "      support_data(numpy.array): the support data, output of Data_loader::next_batch_gen()(x[0])\n",
    "      query_data_x(numpy.array): the query data, output of Data_loader::next_batch_gen()(x[1])\n",
    "      query_data_y(numpy.array): the query data label, output of Data_loader::next_batch_gen(y[0])\n",
    "\n",
    "    Returns:\n",
    "      x_support(torch.Tensor): the support shape: (n_class, n_support, *data dimension)\n",
    "      x_query(torch.Tensor): the query set, shape: (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "      y_query(torch.longTensor): the label for query set (n_class, n_query, 1), [positive, negative]\n",
    "    \"\"\"\n",
    "\n",
    "    # support data\n",
    "    x_support_pos = torch.Tensor(support_data[:, :n_shot, :, :].reshape(-1, *support_data.shape[2:]))\n",
    "    x_support_neg = torch.Tensor(support_data[:, n_shot:, :, :].reshape(-1, *support_data.shape[2:]))\n",
    "    x_support = torch.stack([x_support_pos, x_support_neg])\n",
    "\n",
    "    # query data\n",
    "    x_query = torch.Tensor(query_data_x.reshape(2, -1, *query_data_x.shape[1:]))\n",
    "\n",
    "    # y_query\n",
    "    if predict_mode is False:\n",
    "        y_query = Variable(torch.LongTensor(query_data_y).view(2, -1, 1), requires_grad=False)\n",
    "    else:\n",
    "        y_query = None\n",
    "\n",
    "    return x_support, x_query, y_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Base_LSTM_CNN                            --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "├─LSTM: 1-2                              [32, 100, 200]            161,600\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [32, 32, 100, 1]          6,432\n",
       "│    └─Conv2d: 2-2                       [32, 32, 99, 1]           12,832\n",
       "│    └─Conv2d: 2-3                       [32, 32, 98, 1]           19,232\n",
       "├─Dropout: 1-3                           [32, 96]                  --\n",
       "==========================================================================================\n",
       "Total params: 200,096\n",
       "Trainable params: 200,096\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 638.67\n",
       "==========================================================================================\n",
       "Input size (MB): 1.28\n",
       "Forward/backward pass size (MB): 7.55\n",
       "Params size (MB): 0.80\n",
       "Estimated Total Size (MB): 9.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Base_LSTM_CNN(nn.Module):\n",
    "\n",
    "    # define all the layers used in model\n",
    "    def __init__(self, emb_dim, seq_len, lstm_units, num_filters, kernel_sizes, num_classes, dropout_rate = 0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_units = lstm_units\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
    "        self.lstm = nn.LSTM(emb_dim,\n",
    "                            lstm_units,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.num_filters, (f, 2*self.lstm_units)) for f in self.kernel_sizes])\n",
    "        # self.fc = nn.Linear(len(kernel_sizes)*self.num_filters, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # (N, seq_len, 2*lstm_units)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.size())\n",
    "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
    "        # print(x[0].size())\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
    "        x = torch.cat(x, 1)  # N, len(filter_sizes)* num_filters\n",
    "        #print(x.size())\n",
    "        x = self.dropout(x)  # N, len(filter_sizes)* num_filters\n",
    "        # logit = self.fc(x)  # (N, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Base_LSTM_CNN(100, 200, 100, 32, [1,2,3], 2)\n",
    "summary(model,(32, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Base_CNN                                 --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [32, 32, 100, 1]          24,608\n",
       "│    └─Conv2d: 2-2                       [32, 32, 98, 1]           73,760\n",
       "│    └─Conv2d: 2-3                       [32, 32, 96, 1]           122,912\n",
       "==========================================================================================\n",
       "Total params: 221,280\n",
       "Trainable params: 221,280\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 687.64\n",
       "==========================================================================================\n",
       "Input size (MB): 9.83\n",
       "Forward/backward pass size (MB): 2.41\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 13.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Base_CNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_filter, kernel_sizes):\n",
    "        # initialization\n",
    "        super(Base_CNN, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_filter = num_filter\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        # convolution layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.num_filter, (f, self.emb_dim)) for f in self.kernel_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "cnn_base_model_1 = Base_CNN(emb_dim=768, num_filter=32, kernel_sizes=[1,3,5])\n",
    "summary(cnn_base_model_1, (32, 100, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "class Protonet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(Protonet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_dist(x, y):\n",
    "        n = x.size(0)\n",
    "        m = y.size(0)\n",
    "        d = x.size(1)\n",
    "        assert d == y.size(1)\n",
    "\n",
    "        x = x.unsqueeze(1).expand(n, m, d)\n",
    "        y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "        return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "    def forward(self, x_support, x_query, y_query):\n",
    "        \"\"\"\n",
    "        Forward for prototypical network\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "          y_query(torch.longTensor): the label for query set (n_class, n_query, 1)\n",
    "\n",
    "        returns:\n",
    "          loss(torch.Tensor): negative log likelihood to be minimized, will be used to update the parameters via backprobagation\n",
    "          acc(float): accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # loss\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        loss = -log_p_y.gather(2, y_query).squeeze().view(-1).mean()  # loss\n",
    "        _, y_hat = log_p_y.max(2) # return: max, max_indicies\n",
    "        acc = torch.eq(y_hat, y_query.squeeze()).float().mean().item()\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def evaluation(self, x_support, x_query, y_query):\n",
    "        \"\"\"\n",
    "        evaluation function, will return classes, accuracy, loss(float, requires_grad=False)\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "          y_query(torch.longTensor): the label for query set (n_class, n_query, 1)\n",
    "\n",
    "        returns:\n",
    "          y_hat(torch.Tensor): the prediction label\n",
    "          loss(float): loss\n",
    "          acc(float): accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # loss\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        loss = -log_p_y.gather(2, y_query).squeeze().view(-1).mean().item()  # loss\n",
    "        _, y_hat = log_p_y.max(2) # return: max, max_indicies\n",
    "        acc = torch.eq(y_hat, y_query.squeeze()).float().mean().item()\n",
    "\n",
    "        return y_hat, acc, loss\n",
    "\n",
    "    def predict_prob(self, x_support, x_query):\n",
    "        \"\"\"\n",
    "\n",
    "        make a prediction, return positive probability\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "\n",
    "        returns:\n",
    "          p_y(float): the average probability\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # probability\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        p_y = torch.exp(log_p_y).view(-1, 2).mean(dim=0) # average probability\n",
    "\n",
    "        return p_y[0].item()\n",
    "        # return p_y[1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train_proto(the_model,\n",
    "                n_epochs,\n",
    "                n_eposides_train,\n",
    "                n_eposides_valid,\n",
    "                learning_rate,\n",
    "                train_loader_generator,\n",
    "                valid_loader_generator,\n",
    "                n_shot,\n",
    "                the_device,\n",
    "                learning_rate_schedule=False,\n",
    "                update_step=100,\n",
    "                gamma=0.5,\n",
    "                save_model=False,\n",
    "                save_path=None,\n",
    "                estimate_theta_subsample_size=1000,\n",
    "                verbose=True):\n",
    "\n",
    "    # move model\n",
    "    the_model = the_model.to(the_device)\n",
    "\n",
    "    # history\n",
    "    history = {'train_acc': [], 'train_loss': [],\n",
    "               'train_acc_avg': [], 'train_loss_avg': [],\n",
    "               'valid_acc': [], 'valid_loss': [],\n",
    "               'valid_acc_avg': [], 'valid_loss_avg': []}\n",
    "\n",
    "    # set up optimizer\n",
    "    optimizer = torch.optim.Adam(the_model.parameters(), lr=learning_rate)\n",
    "    # if schedule learning rate\n",
    "    if learning_rate_schedule is True:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                    step_size=update_step,\n",
    "                                                    gamma=gamma)\n",
    "        update_count = 0\n",
    "        if verbose:\n",
    "            print(f\"Initial learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "    # epoch loop\n",
    "    for i in range(n_epochs):\n",
    "        # 1. estimate theta\n",
    "        # data loader\n",
    "        cur_theta_estimate_dataloader = train_loader_generator.batch_generator_without_theta(subsample_size=estimate_theta_subsample_size)\n",
    "        theta_pred = []\n",
    "        theta_label = []\n",
    "        theta_diffs = []\n",
    "        # set model to eval\n",
    "        model.eval()\n",
    "        # evaluate sampled data\n",
    "        for cur_support_data, cur_query_data_x, cur_query_data_y, cur_diff in cur_theta_estimate_dataloader:\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # evaluation\n",
    "            y_hat, acc, loss = the_model.evaluation(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # metrics\n",
    "            theta_pred.extend(y_hat.view(-1).long().cpu().numpy().tolist())\n",
    "            theta_label.extend(cur_query_data_y.tolist())\n",
    "            theta_diffs.extend(cur_diff.numpy().tolist())\n",
    "        # evaluate theta\n",
    "        theta_diffs = np.array(theta_diffs)\n",
    "        theta_pred = np.array(theta_pred)\n",
    "        theta_label = np.array(theta_label)\n",
    "        theta_resp = np.where(theta_pred == theta_label, 1, 0)\n",
    "        cur_theta = scoring.calculate_theta(difficulties=theta_diffs, response_pattern=theta_resp)[0]\n",
    "\n",
    "        # 2. train\n",
    "        the_model.train()\n",
    "        train_cumulative_acc = 0\n",
    "        train_cumulative_loss = 0\n",
    "        cur_train_loader, cur_train_dataset_len = train_loader_generator.batch_generator_with_theta(theta=cur_theta)\n",
    "        for _ in range(n_eposides_train):\n",
    "            # retrieve data from train loader\n",
    "            cur_support_data, cur_query_data_x, cur_query_data_y, _ = cur_train_loader.next_batch()\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # forward\n",
    "            cur_loss, cur_acc = the_model(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # backward\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # learning rate schedule\n",
    "            if learning_rate_schedule is True:\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "                if update_count % update_step == 0 and verbose:\n",
    "                    print(f\"Update learning rate to {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "            # cumulative loss and acc\n",
    "            train_cumulative_acc += cur_acc\n",
    "            train_cumulative_loss += cur_loss.item()\n",
    "            # record\n",
    "            history['train_acc'].append(cur_acc)\n",
    "            history['train_loss'].append(cur_loss.item())\n",
    "\n",
    "        # 3. evaluation\n",
    "        valid_cumulative_acc = 0\n",
    "        valid_cumulative_loss = 0\n",
    "        cur_valid_loader = valid_loader_generator.batch_generator_without_theta(valid=True)\n",
    "        for _ in range(n_eposides_valid):\n",
    "            the_model.eval()\n",
    "            # retrieve data from validation loader\n",
    "            cur_support_data, cur_query_data_x, cur_query_data_y = cur_valid_loader.next_batch()\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # evaluation\n",
    "            y_hat, acc, loss = the_model.evaluation(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # metrics\n",
    "            valid_cumulative_acc += acc\n",
    "            valid_cumulative_loss += loss\n",
    "            # record\n",
    "            history['valid_acc'].append(acc)\n",
    "            history['valid_loss'].append(loss)\n",
    "\n",
    "        # record\n",
    "        history['train_acc_avg'].append(train_cumulative_acc / n_eposides_train)\n",
    "        history['train_loss_avg'].append(train_cumulative_loss / n_eposides_train)\n",
    "        history['valid_acc_avg'].append(valid_cumulative_acc / n_eposides_valid)\n",
    "        history['valid_loss_avg'].append(valid_cumulative_loss / n_eposides_valid)\n",
    "\n",
    "        # verbose\n",
    "        if verbose:\n",
    "            print('=' * 10 + f'Epoch: {i + 1} / {n_epochs}' + '=' * 10)\n",
    "            print(f'\\nTrain acc: {train_cumulative_acc / n_eposides_train}, Train loss: {train_cumulative_loss / n_eposides_train}')\n",
    "            print(f'\\nValidation acc: {valid_cumulative_acc / n_eposides_valid}, Validation loss: {valid_cumulative_loss / n_eposides_valid}')\n",
    "            print(f'\\nCurrent Theta: {cur_theta}')\n",
    "            print(f'\\nCurrent Train Dataset Length: {cur_train_dataset_len}')\n",
    "            print('\\n')\n",
    "            print('=' * 37)\n",
    "\n",
    "    # save model\n",
    "    if save_model:\n",
    "        torch.save(the_model.state_dict(), save_path)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = IRT_train['Perspective']\n",
    "valid_df = IRT_eval[\"Perspective\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005edd-1296-4630-9789-b5a5a23972f1</td>\n",
       "      <td>[6, 225, 181, 0, 211]</td>\n",
       "      <td>[596, 207, 117, 322, 249]</td>\n",
       "      <td>417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.371309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001c7fa-751f-478a-9e04-bfc836a0a514</td>\n",
       "      <td>[4, 66, 181, 179, 11]</td>\n",
       "      <td>[111, 177, 634, 508, 630]</td>\n",
       "      <td>292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.408183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00036e5c-f52d-4c1c-a88e-bae07525c43a</td>\n",
       "      <td>[34, 31, 35, 211, 58]</td>\n",
       "      <td>[443, 488, 583, 241, 268]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.038628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061dae-95a0-4afe-aee0-fb4baf6be08f</td>\n",
       "      <td>[225, 7, 164, 39, 33]</td>\n",
       "      <td>[83, 347, 186, 424, 314]</td>\n",
       "      <td>385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.268913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008324d-24a1-4489-922d-2ddff8a7519f</td>\n",
       "      <td>[5, 3, 67, 11, 51]</td>\n",
       "      <td>[449, 318, 506, 596, 503]</td>\n",
       "      <td>651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.429216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34990</th>\n",
       "      <td>fff78c18-c9aa-455f-883f-33c8beead374</td>\n",
       "      <td>[10, 179, 153, 11, 0]</td>\n",
       "      <td>[381, 532, 201, 259, 158]</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.606842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34991</th>\n",
       "      <td>fff955d3-eccc-4976-9e02-c5a403773555</td>\n",
       "      <td>[44, 72, 216, 204, 0]</td>\n",
       "      <td>[148, 159, 120, 263, 556]</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.529471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34992</th>\n",
       "      <td>fff975f8-ebd8-4acb-9367-408cfa1ce09b</td>\n",
       "      <td>[31, 35, 55, 0, 72]</td>\n",
       "      <td>[521, 80, 605, 301, 137]</td>\n",
       "      <td>641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.416260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34993</th>\n",
       "      <td>fffd3a56-1a65-45e4-8dc8-9430cb3f94de</td>\n",
       "      <td>[68, 187, 43, 0, 198]</td>\n",
       "      <td>[435, 146, 88, 477, 393]</td>\n",
       "      <td>564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.764696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34994</th>\n",
       "      <td>fffec17f-253c-4caa-a63f-ea9f4cc450c3</td>\n",
       "      <td>[41, 217, 11, 5, 12]</td>\n",
       "      <td>[617, 377, 298, 654, 414]</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.529369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34995 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID       Pos_support_locs  \\\n",
       "0      00005edd-1296-4630-9789-b5a5a23972f1  [6, 225, 181, 0, 211]   \n",
       "1      0001c7fa-751f-478a-9e04-bfc836a0a514  [4, 66, 181, 179, 11]   \n",
       "2      00036e5c-f52d-4c1c-a88e-bae07525c43a  [34, 31, 35, 211, 58]   \n",
       "3      00061dae-95a0-4afe-aee0-fb4baf6be08f  [225, 7, 164, 39, 33]   \n",
       "4      0008324d-24a1-4489-922d-2ddff8a7519f     [5, 3, 67, 11, 51]   \n",
       "...                                     ...                    ...   \n",
       "34990  fff78c18-c9aa-455f-883f-33c8beead374  [10, 179, 153, 11, 0]   \n",
       "34991  fff955d3-eccc-4976-9e02-c5a403773555  [44, 72, 216, 204, 0]   \n",
       "34992  fff975f8-ebd8-4acb-9367-408cfa1ce09b    [31, 35, 55, 0, 72]   \n",
       "34993  fffd3a56-1a65-45e4-8dc8-9430cb3f94de  [68, 187, 43, 0, 198]   \n",
       "34994  fffec17f-253c-4caa-a63f-ea9f4cc450c3   [41, 217, 11, 5, 12]   \n",
       "\n",
       "                Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0      [596, 207, 117, 322, 249]        417    0.0      1         3 -2.371309  \n",
       "1      [111, 177, 634, 508, 630]        292    0.0      3         1 -2.408183  \n",
       "2      [443, 488, 583, 241, 268]          0    1.0      5         4  1.038628  \n",
       "3       [83, 347, 186, 424, 314]        385    0.0      0         4  0.268913  \n",
       "4      [449, 318, 506, 596, 503]        651    0.0      4         1 -2.429216  \n",
       "...                          ...        ...    ...    ...       ...       ...  \n",
       "34990  [381, 532, 201, 259, 158]          8    1.0      3         4  1.606842  \n",
       "34991  [148, 159, 120, 263, 556]         13    1.0      0         4 -1.529471  \n",
       "34992   [521, 80, 605, 301, 137]        641    0.0      1         1 -2.416260  \n",
       "34993   [435, 146, 88, 477, 393]        564    0.0      4         2 -1.764696  \n",
       "34994  [617, 377, 298, 654, 414]          9    1.0      3         4 -1.529369  \n",
       "\n",
       "[34995 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002b905-1318-4a7a-b182-6bb79663e886</td>\n",
       "      <td>[162, 8, 204, 179, 22]</td>\n",
       "      <td>[306, 145, 342, 331, 490]</td>\n",
       "      <td>117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003fe41-aa46-4334-985c-d3e60ee43e73</td>\n",
       "      <td>[4, 36, 57, 5, 13]</td>\n",
       "      <td>[435, 121, 472, 190, 620]</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006f5e9-e00a-41f6-81f4-484cb153a0f5</td>\n",
       "      <td>[25, 15, 28, 35, 162]</td>\n",
       "      <td>[437, 126, 583, 628, 634]</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000bb31e-883e-4a48-9c70-752ad9f23d99</td>\n",
       "      <td>[11, 58, 187, 57, 28]</td>\n",
       "      <td>[564, 220, 333, 169, 277]</td>\n",
       "      <td>212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000bb35c-7762-4af1-bdb7-e7373ffecda1</td>\n",
       "      <td>[47, 68, 216, 4, 15]</td>\n",
       "      <td>[171, 421, 155, 447, 593]</td>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>fff38f50-782b-4ac2-b949-bcab0f116ccf</td>\n",
       "      <td>[68, 204, 34, 164, 67]</td>\n",
       "      <td>[582, 249, 511, 573, 91]</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>fff3e072-6abd-48f4-8830-ea7a7f47e862</td>\n",
       "      <td>[8, 214, 64, 3, 57]</td>\n",
       "      <td>[615, 524, 433, 104, 367]</td>\n",
       "      <td>161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>fff72ad5-39fd-41be-93be-6cb6bea91ee0</td>\n",
       "      <td>[211, 179, 27, 13, 64]</td>\n",
       "      <td>[279, 359, 246, 239, 288]</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>fffc13f0-55fb-4df9-9ff2-443bc63934be</td>\n",
       "      <td>[225, 13, 19, 167, 67]</td>\n",
       "      <td>[251, 405, 380, 580, 446]</td>\n",
       "      <td>207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>fffd5d68-50c7-4b51-a07b-08b1ecd01812</td>\n",
       "      <td>[17, 72, 40, 68, 181]</td>\n",
       "      <td>[119, 584, 419, 558, 180]</td>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14981 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID        Pos_support_locs  \\\n",
       "0      0002b905-1318-4a7a-b182-6bb79663e886  [162, 8, 204, 179, 22]   \n",
       "1      0003fe41-aa46-4334-985c-d3e60ee43e73      [4, 36, 57, 5, 13]   \n",
       "2      0006f5e9-e00a-41f6-81f4-484cb153a0f5   [25, 15, 28, 35, 162]   \n",
       "3      000bb31e-883e-4a48-9c70-752ad9f23d99   [11, 58, 187, 57, 28]   \n",
       "4      000bb35c-7762-4af1-bdb7-e7373ffecda1    [47, 68, 216, 4, 15]   \n",
       "...                                     ...                     ...   \n",
       "14976  fff38f50-782b-4ac2-b949-bcab0f116ccf  [68, 204, 34, 164, 67]   \n",
       "14977  fff3e072-6abd-48f4-8830-ea7a7f47e862     [8, 214, 64, 3, 57]   \n",
       "14978  fff72ad5-39fd-41be-93be-6cb6bea91ee0  [211, 179, 27, 13, 64]   \n",
       "14979  fffc13f0-55fb-4df9-9ff2-443bc63934be  [225, 13, 19, 167, 67]   \n",
       "14980  fffd5d68-50c7-4b51-a07b-08b1ecd01812   [17, 72, 40, 68, 181]   \n",
       "\n",
       "                Neg_support_locs  Query_loc  Label  Alpha  Aug_type  \n",
       "0      [306, 145, 342, 331, 490]        117    0.0      0         0  \n",
       "1      [435, 121, 472, 190, 620]         73    1.0      0         0  \n",
       "2      [437, 126, 583, 628, 634]         53    1.0      0         0  \n",
       "3      [564, 220, 333, 169, 277]        212    1.0      0         0  \n",
       "4      [171, 421, 155, 447, 593]        272    0.0      0         0  \n",
       "...                          ...        ...    ...    ...       ...  \n",
       "14976   [582, 249, 511, 573, 91]         70    1.0      0         0  \n",
       "14977  [615, 524, 433, 104, 367]        161    0.0      0         0  \n",
       "14978  [279, 359, 246, 239, 288]         37    1.0      0         0  \n",
       "14979  [251, 405, 380, 580, 446]        207    0.0      0         0  \n",
       "14980  [119, 584, 419, 558, 180]         91    0.0      0         0  \n",
       "\n",
       "[14981 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['diff'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch: 1 / 5==========\n",
      "\n",
      "Train acc: 0.81884765625, Train loss: 1.1052744695916772\n",
      "\n",
      "Validation acc: 0.900390625, Validation loss: 0.32687396614346653\n",
      "\n",
      "Current Theta: 2.7997656250000107\n",
      "\n",
      "Current Train Dataset Length: 30701\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 2 / 5==========\n",
      "\n",
      "Train acc: 0.931640625, Train loss: 0.18652014143299311\n",
      "\n",
      "Validation acc: 0.84033203125, Validation loss: 0.5476232464425266\n",
      "\n",
      "Current Theta: 3.2928125000000117\n",
      "\n",
      "Current Train Dataset Length: 31649\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 3 / 5==========\n",
      "\n",
      "Train acc: 0.9228515625, Train loss: 0.2082398529164493\n",
      "\n",
      "Validation acc: 0.8076171875, Validation loss: 0.48290606308728456\n",
      "\n",
      "Current Theta: 2.9301562500000102\n",
      "\n",
      "Current Train Dataset Length: 30969\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 4 / 5==========\n",
      "\n",
      "Train acc: 0.94287109375, Train loss: 0.1554584497353062\n",
      "\n",
      "Validation acc: 0.87939453125, Validation loss: 0.3670262536033988\n",
      "\n",
      "Current Theta: 3.359765625000013\n",
      "\n",
      "Current Train Dataset Length: 31760\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 5 / 5==========\n",
      "\n",
      "Train acc: 0.95458984375, Train loss: 0.1450735287507996\n",
      "\n",
      "Validation acc: 0.9521484375, Validation loss: 0.2733296407095622\n",
      "\n",
      "Current Theta: 3.335390625000012\n",
      "\n",
      "Current Train Dataset Length: 31704\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.625,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.859375,\n",
       "  0.734375,\n",
       "  0.5625,\n",
       "  0.734375,\n",
       "  0.78125,\n",
       "  0.71875,\n",
       "  0.828125,\n",
       "  0.703125,\n",
       "  0.6875,\n",
       "  0.515625,\n",
       "  0.890625,\n",
       "  0.8125,\n",
       "  0.78125,\n",
       "  0.8125,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.75,\n",
       "  0.859375,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.84375,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.90625,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.796875,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.796875,\n",
       "  0.921875,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.9375,\n",
       "  0.921875],\n",
       " 'train_loss': [0.6702514886856079,\n",
       "  2.6177494525909424,\n",
       "  8.126755714416504,\n",
       "  1.6588869094848633,\n",
       "  3.93648624420166,\n",
       "  5.229011535644531,\n",
       "  1.2704392671585083,\n",
       "  1.0809061527252197,\n",
       "  1.312579870223999,\n",
       "  0.4033803939819336,\n",
       "  0.5867854356765747,\n",
       "  0.551833987236023,\n",
       "  1.727623462677002,\n",
       "  0.3009016513824463,\n",
       "  0.8008549213409424,\n",
       "  0.5649421215057373,\n",
       "  0.4924699366092682,\n",
       "  0.348533034324646,\n",
       "  0.3684854507446289,\n",
       "  0.5140514373779297,\n",
       "  0.2655414938926697,\n",
       "  0.21607407927513123,\n",
       "  0.2609339952468872,\n",
       "  0.2707580327987671,\n",
       "  0.32655662298202515,\n",
       "  0.23291854560375214,\n",
       "  0.25366345047950745,\n",
       "  0.2326984703540802,\n",
       "  0.11476185917854309,\n",
       "  0.2266080677509308,\n",
       "  0.2512813210487366,\n",
       "  0.15405862033367157,\n",
       "  0.3172914683818817,\n",
       "  0.18093271553516388,\n",
       "  0.22470694780349731,\n",
       "  0.4722288250923157,\n",
       "  0.33911454677581787,\n",
       "  0.17503315210342407,\n",
       "  0.177372545003891,\n",
       "  0.176631361246109,\n",
       "  0.22642505168914795,\n",
       "  0.21731093525886536,\n",
       "  0.13367176055908203,\n",
       "  0.11477431654930115,\n",
       "  0.3788996636867523,\n",
       "  0.10326103866100311,\n",
       "  0.203603595495224,\n",
       "  0.17820776998996735,\n",
       "  0.1625380516052246,\n",
       "  0.23598532378673553,\n",
       "  0.05247120186686516,\n",
       "  0.0817340686917305,\n",
       "  0.18112041056156158,\n",
       "  0.14835555851459503,\n",
       "  0.10506695508956909,\n",
       "  0.1718062460422516,\n",
       "  0.1942734271287918,\n",
       "  0.20935148000717163,\n",
       "  0.08145444840192795,\n",
       "  0.13348031044006348,\n",
       "  0.12159833312034607,\n",
       "  0.1894594430923462,\n",
       "  0.12608632445335388,\n",
       "  0.15439724922180176,\n",
       "  0.13297680020332336,\n",
       "  0.1525498926639557,\n",
       "  0.08531170338392258,\n",
       "  0.07071743905544281,\n",
       "  0.16119384765625,\n",
       "  0.3536521792411804,\n",
       "  0.130382239818573,\n",
       "  0.20173633098602295,\n",
       "  0.17259269952774048,\n",
       "  0.22459197044372559,\n",
       "  0.1320645809173584,\n",
       "  0.06053280085325241,\n",
       "  0.35449960827827454,\n",
       "  0.2635280191898346,\n",
       "  0.3985368013381958,\n",
       "  0.14234061539173126,\n",
       "  0.19592241942882538,\n",
       "  0.5594218969345093,\n",
       "  0.1780845820903778,\n",
       "  0.2747088074684143,\n",
       "  0.2300165295600891,\n",
       "  0.21757033467292786,\n",
       "  0.21852442622184753,\n",
       "  0.3199846148490906,\n",
       "  0.08275850117206573,\n",
       "  0.1278294026851654,\n",
       "  0.15773606300354004,\n",
       "  0.2329663634300232,\n",
       "  0.3067915141582489,\n",
       "  0.24120303988456726,\n",
       "  0.12556293606758118,\n",
       "  0.15738633275032043,\n",
       "  0.18651807308197021,\n",
       "  0.13695459067821503,\n",
       "  0.17470553517341614,\n",
       "  0.14935874938964844,\n",
       "  0.29567623138427734,\n",
       "  0.08380547910928726,\n",
       "  0.06978117674589157,\n",
       "  0.14662690460681915,\n",
       "  0.08903872221708298,\n",
       "  0.10937169194221497,\n",
       "  0.08933991193771362,\n",
       "  0.05895387753844261,\n",
       "  0.14949825406074524,\n",
       "  0.1060856506228447,\n",
       "  0.09683224558830261,\n",
       "  0.09490380436182022,\n",
       "  0.4321758449077606,\n",
       "  0.09822317957878113,\n",
       "  0.23663054406642914,\n",
       "  0.14847446978092194,\n",
       "  0.21744415163993835,\n",
       "  0.06308414041996002,\n",
       "  0.08219718933105469,\n",
       "  0.4477204382419586,\n",
       "  0.18087667226791382,\n",
       "  0.11651083827018738,\n",
       "  0.14584797620773315,\n",
       "  0.19168683886528015,\n",
       "  0.1443111151456833,\n",
       "  0.15053412318229675,\n",
       "  0.12136872857809067,\n",
       "  0.1601332426071167,\n",
       "  0.09822173416614532,\n",
       "  0.19787506759166718,\n",
       "  0.31657740473747253,\n",
       "  0.13544562458992004,\n",
       "  0.11408106982707977,\n",
       "  0.08064751327037811,\n",
       "  0.11093844473361969,\n",
       "  0.07602538168430328,\n",
       "  0.16107383370399475,\n",
       "  0.17433016002178192,\n",
       "  0.12550924718379974,\n",
       "  0.05465751513838768,\n",
       "  0.06110679730772972,\n",
       "  0.16974490880966187,\n",
       "  0.12247344851493835,\n",
       "  0.052847035229206085,\n",
       "  0.09809550642967224,\n",
       "  0.16243213415145874,\n",
       "  0.17250312864780426,\n",
       "  0.05354522913694382,\n",
       "  0.22175267338752747,\n",
       "  0.16930250823497772,\n",
       "  0.04720271751284599,\n",
       "  0.18911439180374146,\n",
       "  0.022968977689743042,\n",
       "  0.10612836480140686,\n",
       "  0.07590291649103165,\n",
       "  0.138950377702713,\n",
       "  0.21189336478710175,\n",
       "  0.47103384137153625,\n",
       "  0.19447903335094452,\n",
       "  0.25549256801605225],\n",
       " 'train_acc_avg': [0.81884765625,\n",
       "  0.931640625,\n",
       "  0.9228515625,\n",
       "  0.94287109375,\n",
       "  0.95458984375],\n",
       " 'train_loss_avg': [1.1052744695916772,\n",
       "  0.18652014143299311,\n",
       "  0.2082398529164493,\n",
       "  0.1554584497353062,\n",
       "  0.1450735287507996],\n",
       " 'valid_acc': [0.859375,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  1.0,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.90625,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.828125,\n",
       "  0.828125,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.75,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.703125,\n",
       "  0.8125,\n",
       "  0.78125,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.78125,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.84375,\n",
       "  0.828125,\n",
       "  0.671875,\n",
       "  0.859375,\n",
       "  0.875,\n",
       "  0.78125,\n",
       "  0.796875,\n",
       "  0.75,\n",
       "  0.78125,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.734375,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.796875,\n",
       "  0.8125,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.859375,\n",
       "  0.640625,\n",
       "  0.734375,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.84375,\n",
       "  0.765625,\n",
       "  0.71875,\n",
       "  0.90625,\n",
       "  0.703125,\n",
       "  0.765625,\n",
       "  0.890625,\n",
       "  0.828125,\n",
       "  0.859375,\n",
       "  0.71875,\n",
       "  0.828125,\n",
       "  0.78125,\n",
       "  0.796875,\n",
       "  0.75,\n",
       "  0.890625,\n",
       "  0.75,\n",
       "  0.84375,\n",
       "  0.890625,\n",
       "  0.8125,\n",
       "  0.921875,\n",
       "  0.765625,\n",
       "  0.671875,\n",
       "  0.796875,\n",
       "  0.6875,\n",
       "  0.796875,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.859375,\n",
       "  0.96875,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.796875,\n",
       "  0.78125,\n",
       "  0.6875,\n",
       "  0.984375,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.796875,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.796875,\n",
       "  0.9375,\n",
       "  0.765625,\n",
       "  0.96875,\n",
       "  0.78125,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.984375],\n",
       " 'valid_loss': [0.38719823956489563,\n",
       "  0.3462311029434204,\n",
       "  0.344009131193161,\n",
       "  0.09001509845256805,\n",
       "  0.16467517614364624,\n",
       "  0.5582501292228699,\n",
       "  0.34650909900665283,\n",
       "  0.44256874918937683,\n",
       "  0.24313250184059143,\n",
       "  0.3716995418071747,\n",
       "  0.15311896800994873,\n",
       "  0.18174229562282562,\n",
       "  0.3030790686607361,\n",
       "  0.4763130843639374,\n",
       "  0.39779582619667053,\n",
       "  0.46233779191970825,\n",
       "  0.40169280767440796,\n",
       "  0.4017855226993561,\n",
       "  0.16837139427661896,\n",
       "  0.016339275985956192,\n",
       "  0.2729143798351288,\n",
       "  0.3925396800041199,\n",
       "  0.321582555770874,\n",
       "  0.5137872695922852,\n",
       "  0.36636975407600403,\n",
       "  0.3219575881958008,\n",
       "  0.2683345675468445,\n",
       "  0.4260585904121399,\n",
       "  0.4804746210575104,\n",
       "  0.22035667300224304,\n",
       "  0.29165032505989075,\n",
       "  0.32707610726356506,\n",
       "  0.41914162039756775,\n",
       "  0.6873985528945923,\n",
       "  0.4307441711425781,\n",
       "  0.3715006113052368,\n",
       "  0.6151555180549622,\n",
       "  0.2716415524482727,\n",
       "  0.8303321599960327,\n",
       "  0.6610241532325745,\n",
       "  0.5524197816848755,\n",
       "  0.5713552236557007,\n",
       "  0.5098883509635925,\n",
       "  0.8330137729644775,\n",
       "  0.37851929664611816,\n",
       "  0.4395584166049957,\n",
       "  0.5060275793075562,\n",
       "  0.2844460606575012,\n",
       "  1.1957536935806274,\n",
       "  0.5396613478660583,\n",
       "  0.6721646189689636,\n",
       "  0.7055163979530334,\n",
       "  0.33737602829933167,\n",
       "  0.6285806894302368,\n",
       "  0.8635388612747192,\n",
       "  0.2578229606151581,\n",
       "  0.29268285632133484,\n",
       "  0.7710825204849243,\n",
       "  0.35253602266311646,\n",
       "  0.1579923778772354,\n",
       "  0.7151175141334534,\n",
       "  0.5703956484794617,\n",
       "  0.42360061407089233,\n",
       "  0.677954912185669,\n",
       "  0.3022093176841736,\n",
       "  0.4087713658809662,\n",
       "  0.9118813276290894,\n",
       "  0.5037420988082886,\n",
       "  0.35077768564224243,\n",
       "  0.3271009624004364,\n",
       "  0.3858627676963806,\n",
       "  0.16573873162269592,\n",
       "  0.5005851984024048,\n",
       "  0.5438660979270935,\n",
       "  0.8307665586471558,\n",
       "  0.2771100103855133,\n",
       "  0.5508142709732056,\n",
       "  0.5197139978408813,\n",
       "  0.37646955251693726,\n",
       "  0.5607039332389832,\n",
       "  0.3978240489959717,\n",
       "  0.7669302225112915,\n",
       "  0.515532374382019,\n",
       "  0.39873823523521423,\n",
       "  0.3614356517791748,\n",
       "  0.5658338665962219,\n",
       "  0.45920705795288086,\n",
       "  0.501976728439331,\n",
       "  0.3910105228424072,\n",
       "  0.34437423944473267,\n",
       "  0.39972686767578125,\n",
       "  0.21222716569900513,\n",
       "  0.4717335104942322,\n",
       "  0.7449566721916199,\n",
       "  0.4202399253845215,\n",
       "  0.9851330518722534,\n",
       "  0.7900297045707703,\n",
       "  0.4751843214035034,\n",
       "  0.31704819202423096,\n",
       "  0.3915265202522278,\n",
       "  0.11915579438209534,\n",
       "  0.2647186815738678,\n",
       "  0.378940224647522,\n",
       "  0.5727481245994568,\n",
       "  0.46677765250205994,\n",
       "  0.900554895401001,\n",
       "  0.10630588978528976,\n",
       "  0.36908841133117676,\n",
       "  0.17138098180294037,\n",
       "  0.7294632792472839,\n",
       "  0.21359789371490479,\n",
       "  0.24189242720603943,\n",
       "  0.5124103426933289,\n",
       "  0.37248414754867554,\n",
       "  0.6618516445159912,\n",
       "  0.1303386390209198,\n",
       "  0.5333009958267212,\n",
       "  0.3471459746360779,\n",
       "  0.15064942836761475,\n",
       "  0.11952229589223862,\n",
       "  0.4393477439880371,\n",
       "  0.23385387659072876,\n",
       "  0.26754310727119446,\n",
       "  0.1691364347934723,\n",
       "  0.14457523822784424,\n",
       "  0.4723747968673706,\n",
       "  0.2547778785228729,\n",
       "  0.4271145761013031,\n",
       "  0.21337401866912842,\n",
       "  0.011531347408890724,\n",
       "  0.02827228419482708,\n",
       "  0.11169423907995224,\n",
       "  0.36985743045806885,\n",
       "  0.24667958915233612,\n",
       "  0.3830893039703369,\n",
       "  0.26661619544029236,\n",
       "  0.03495960682630539,\n",
       "  0.4213945269584656,\n",
       "  0.275496244430542,\n",
       "  0.10433576256036758,\n",
       "  0.24246767163276672,\n",
       "  0.7537184357643127,\n",
       "  0.26530036330223083,\n",
       "  0.06786306202411652,\n",
       "  0.625728964805603,\n",
       "  0.18675442039966583,\n",
       "  0.3248957693576813,\n",
       "  0.7535393238067627,\n",
       "  0.2592315971851349,\n",
       "  0.41838154196739197,\n",
       "  0.590547502040863,\n",
       "  0.19570699334144592,\n",
       "  0.005016646347939968,\n",
       "  0.22272273898124695,\n",
       "  0.11739562451839447,\n",
       "  0.6443921327590942,\n",
       "  0.2797245383262634,\n",
       "  0.08144787698984146,\n",
       "  0.1296948343515396,\n",
       "  0.11471791565418243],\n",
       " 'valid_acc_avg': [0.900390625,\n",
       "  0.84033203125,\n",
       "  0.8076171875,\n",
       "  0.87939453125,\n",
       "  0.9521484375],\n",
       " 'valid_loss_avg': [0.32687396614346653,\n",
       "  0.5476232464425266,\n",
       "  0.48290606308728456,\n",
       "  0.3670262536033988,\n",
       "  0.2733296407095622]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "n_shot = 5\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "# data set up\n",
    "train_sampler = CL_dff_loader(data_df=train_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "valid_sampler = CL_dff_loader(data_df=valid_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "# model set up\n",
    "cnn_lstm = Base_LSTM_CNN(emb_dim=768, seq_len=100, lstm_units=200, num_filters=30, kernel_sizes=[1, 3, 5], num_classes=2)\n",
    "prto_model = Protonet(cnn_lstm)\n",
    "\n",
    "train_proto(the_model=prto_model,\n",
    "            n_epochs=n_epochs,\n",
    "            learning_rate=lr,\n",
    "            train_loader_generator=train_sampler,\n",
    "            valid_loader_generator=valid_sampler,\n",
    "            n_eposides_train=32,\n",
    "            n_eposides_valid=32,\n",
    "            n_shot=n_shot,\n",
    "            estimate_theta_subsample_size=50,\n",
    "            the_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch: 1 / 5==========\n",
      "\n",
      "Train acc: 0.76220703125, Train loss: 1.380190558731556\n",
      "\n",
      "Validation acc: 0.95947265625, Validation loss: 0.1796806906349957\n",
      "\n",
      "Current Theta: 6.345390625000022\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 2 / 5==========\n",
      "\n",
      "Train acc: 0.814453125, Train loss: 0.3960344837978482\n",
      "\n",
      "Validation acc: 0.91943359375, Validation loss: 0.23799325851723552\n",
      "\n",
      "Current Theta: 6.548750000000023\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 3 / 5==========\n",
      "\n",
      "Train acc: 0.8447265625, Train loss: 0.3402380612678826\n",
      "\n",
      "Validation acc: 0.95166015625, Validation loss: 0.19821013626642525\n",
      "\n",
      "Current Theta: 7.146953125000024\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 4 / 5==========\n",
      "\n",
      "Train acc: 0.8447265625, Train loss: 0.3876847978681326\n",
      "\n",
      "Validation acc: 0.97021484375, Validation loss: 0.09395551937632263\n",
      "\n",
      "Current Theta: 7.040625000000025\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 5 / 5==========\n",
      "\n",
      "Train acc: 0.8759765625, Train loss: 0.32670884067192674\n",
      "\n",
      "Validation acc: 0.6650390625, Validation loss: 2.0795675180852413\n",
      "\n",
      "Current Theta: 7.100234375000024\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.640625,\n",
       "  0.78125,\n",
       "  0.734375,\n",
       "  0.6875,\n",
       "  0.734375,\n",
       "  0.625,\n",
       "  0.578125,\n",
       "  0.609375,\n",
       "  0.671875,\n",
       "  0.6875,\n",
       "  0.6875,\n",
       "  0.640625,\n",
       "  0.78125,\n",
       "  0.828125,\n",
       "  0.71875,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.765625,\n",
       "  0.875,\n",
       "  0.84375,\n",
       "  0.78125,\n",
       "  0.765625,\n",
       "  0.859375,\n",
       "  0.796875,\n",
       "  0.90625,\n",
       "  0.78125,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.78125,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.71875,\n",
       "  0.8125,\n",
       "  0.84375,\n",
       "  0.75,\n",
       "  0.8125,\n",
       "  0.78125,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.828125,\n",
       "  0.78125,\n",
       "  0.859375,\n",
       "  0.828125,\n",
       "  0.8125,\n",
       "  0.828125,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.71875,\n",
       "  0.859375,\n",
       "  0.84375,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.8125,\n",
       "  0.859375,\n",
       "  0.75,\n",
       "  0.6875,\n",
       "  0.8125,\n",
       "  0.78125,\n",
       "  0.703125,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.859375,\n",
       "  0.765625,\n",
       "  0.75,\n",
       "  0.765625,\n",
       "  0.828125,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.796875,\n",
       "  0.765625,\n",
       "  0.859375,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.828125,\n",
       "  0.703125,\n",
       "  0.734375,\n",
       "  0.78125,\n",
       "  0.859375,\n",
       "  0.921875,\n",
       "  0.828125,\n",
       "  0.921875,\n",
       "  0.8125,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.765625,\n",
       "  0.828125,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.734375,\n",
       "  0.84375,\n",
       "  0.90625,\n",
       "  0.796875,\n",
       "  0.875,\n",
       "  0.796875,\n",
       "  0.859375,\n",
       "  0.765625,\n",
       "  0.859375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.796875,\n",
       "  0.90625,\n",
       "  0.765625,\n",
       "  0.859375,\n",
       "  0.828125,\n",
       "  0.921875,\n",
       "  0.796875,\n",
       "  0.859375,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.8125,\n",
       "  0.75,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.796875,\n",
       "  0.828125,\n",
       "  0.9375,\n",
       "  0.859375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.859375],\n",
       " 'train_loss': [0.6667090058326721,\n",
       "  9.345590591430664,\n",
       "  2.933626890182495,\n",
       "  7.840057849884033,\n",
       "  3.7402420043945312,\n",
       "  4.275313377380371,\n",
       "  2.9686989784240723,\n",
       "  0.9622052907943726,\n",
       "  0.9557026028633118,\n",
       "  0.577424168586731,\n",
       "  0.6277300119400024,\n",
       "  0.6811099052429199,\n",
       "  0.5645686388015747,\n",
       "  0.32898905873298645,\n",
       "  0.5497437715530396,\n",
       "  0.4931899905204773,\n",
       "  0.526779055595398,\n",
       "  0.4985651969909668,\n",
       "  0.5670690536499023,\n",
       "  0.3798801898956299,\n",
       "  0.4666350483894348,\n",
       "  0.5225157737731934,\n",
       "  0.36479824781417847,\n",
       "  0.4934448301792145,\n",
       "  0.2976471781730652,\n",
       "  0.45991218090057373,\n",
       "  0.29730337858200073,\n",
       "  0.32080841064453125,\n",
       "  0.43111318349838257,\n",
       "  0.29668548703193665,\n",
       "  0.39163312315940857,\n",
       "  0.3404054045677185,\n",
       "  0.3255946636199951,\n",
       "  0.3222912549972534,\n",
       "  0.2857937216758728,\n",
       "  0.5902323722839355,\n",
       "  0.3953317701816559,\n",
       "  0.37310174107551575,\n",
       "  0.5488184690475464,\n",
       "  0.46403902769088745,\n",
       "  0.41445448994636536,\n",
       "  0.3991144299507141,\n",
       "  0.3476555049419403,\n",
       "  0.4559045732021332,\n",
       "  0.5061396360397339,\n",
       "  0.2897186279296875,\n",
       "  0.3310694396495819,\n",
       "  0.4075215458869934,\n",
       "  0.33571815490722656,\n",
       "  0.3554329574108124,\n",
       "  0.3806111216545105,\n",
       "  0.5572261214256287,\n",
       "  0.2842174768447876,\n",
       "  0.31427857279777527,\n",
       "  0.3428688943386078,\n",
       "  0.33914995193481445,\n",
       "  0.4299309253692627,\n",
       "  0.29528263211250305,\n",
       "  0.3190523386001587,\n",
       "  0.408818781375885,\n",
       "  0.5045456886291504,\n",
       "  0.34635820984840393,\n",
       "  0.40892845392227173,\n",
       "  0.5939019322395325,\n",
       "  0.31479835510253906,\n",
       "  0.34871983528137207,\n",
       "  0.3479904234409332,\n",
       "  0.3695511519908905,\n",
       "  0.2597654163837433,\n",
       "  0.5395548343658447,\n",
       "  0.36465469002723694,\n",
       "  0.40481793880462646,\n",
       "  0.3557608127593994,\n",
       "  0.34039220213890076,\n",
       "  0.3613557815551758,\n",
       "  0.2503977417945862,\n",
       "  0.49121272563934326,\n",
       "  0.5267138481140137,\n",
       "  0.31424057483673096,\n",
       "  0.2981233596801758,\n",
       "  0.39761704206466675,\n",
       "  0.340335488319397,\n",
       "  0.39764246344566345,\n",
       "  0.3130526542663574,\n",
       "  0.43872109055519104,\n",
       "  0.41164422035217285,\n",
       "  0.22856742143630981,\n",
       "  0.3637206554412842,\n",
       "  0.28819626569747925,\n",
       "  0.4015238583087921,\n",
       "  0.27491122484207153,\n",
       "  0.3081226944923401,\n",
       "  0.2764960527420044,\n",
       "  0.2376551330089569,\n",
       "  0.1903235763311386,\n",
       "  0.13103842735290527,\n",
       "  0.4686664938926697,\n",
       "  0.2641783356666565,\n",
       "  0.33834344148635864,\n",
       "  0.7381064891815186,\n",
       "  0.31640422344207764,\n",
       "  0.29841554164886475,\n",
       "  0.20022054016590118,\n",
       "  0.376263290643692,\n",
       "  0.6782749891281128,\n",
       "  0.35574865341186523,\n",
       "  0.24438969790935516,\n",
       "  0.38173866271972656,\n",
       "  0.31660178303718567,\n",
       "  0.3771361708641052,\n",
       "  0.29406416416168213,\n",
       "  0.7348371744155884,\n",
       "  0.3738648295402527,\n",
       "  0.32979297637939453,\n",
       "  0.35662293434143066,\n",
       "  0.3899346590042114,\n",
       "  0.2512063980102539,\n",
       "  0.5995054244995117,\n",
       "  0.3658863306045532,\n",
       "  0.4969606101512909,\n",
       "  0.23314830660820007,\n",
       "  0.42058444023132324,\n",
       "  0.3634106516838074,\n",
       "  0.4014953672885895,\n",
       "  0.33605194091796875,\n",
       "  0.3953394889831543,\n",
       "  0.4783514440059662,\n",
       "  0.23036807775497437,\n",
       "  0.29605263471603394,\n",
       "  0.45064443349838257,\n",
       "  0.24747318029403687,\n",
       "  0.2868560552597046,\n",
       "  0.3117068409919739,\n",
       "  0.2348698228597641,\n",
       "  0.2241906225681305,\n",
       "  0.41968995332717896,\n",
       "  0.3589944839477539,\n",
       "  0.34063416719436646,\n",
       "  0.34918925166130066,\n",
       "  0.3554293215274811,\n",
       "  0.4078623354434967,\n",
       "  0.23459407687187195,\n",
       "  0.3492562174797058,\n",
       "  0.2879096567630768,\n",
       "  0.3641565442085266,\n",
       "  0.41475868225097656,\n",
       "  0.25355643033981323,\n",
       "  0.368685781955719,\n",
       "  0.3566339313983917,\n",
       "  0.32245510816574097,\n",
       "  0.35884320735931396,\n",
       "  0.3062540888786316,\n",
       "  0.29644864797592163,\n",
       "  0.1990271508693695,\n",
       "  0.3831378221511841,\n",
       "  0.41416996717453003,\n",
       "  0.3589666783809662,\n",
       "  0.259701132774353,\n",
       "  0.2994493246078491,\n",
       "  0.3430853486061096],\n",
       " 'train_acc_avg': [0.76220703125,\n",
       "  0.814453125,\n",
       "  0.8447265625,\n",
       "  0.8447265625,\n",
       "  0.8759765625],\n",
       " 'train_loss_avg': [1.380190558731556,\n",
       "  0.3960344837978482,\n",
       "  0.3402380612678826,\n",
       "  0.3876847978681326,\n",
       "  0.32670884067192674],\n",
       " 'valid_acc': [0.9375,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  1.0,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.84375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.796875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.859375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.640625,\n",
       "  0.796875,\n",
       "  0.59375,\n",
       "  0.734375,\n",
       "  0.65625,\n",
       "  0.515625,\n",
       "  0.734375,\n",
       "  0.703125,\n",
       "  0.59375,\n",
       "  0.609375,\n",
       "  0.640625,\n",
       "  0.625,\n",
       "  0.640625,\n",
       "  0.71875,\n",
       "  0.609375,\n",
       "  0.640625,\n",
       "  0.703125,\n",
       "  0.78125,\n",
       "  0.78125,\n",
       "  0.6875,\n",
       "  0.765625,\n",
       "  0.671875,\n",
       "  0.609375,\n",
       "  0.515625,\n",
       "  0.640625,\n",
       "  0.625,\n",
       "  0.65625,\n",
       "  0.78125,\n",
       "  0.6875,\n",
       "  0.625,\n",
       "  0.6875,\n",
       "  0.609375],\n",
       " 'valid_loss': [0.1851341724395752,\n",
       "  0.28346508741378784,\n",
       "  0.1922062486410141,\n",
       "  0.22290483117103577,\n",
       "  0.19610591232776642,\n",
       "  0.19437241554260254,\n",
       "  0.20065255463123322,\n",
       "  0.20761364698410034,\n",
       "  0.15377075970172882,\n",
       "  0.24882616102695465,\n",
       "  0.12017950415611267,\n",
       "  0.17032435536384583,\n",
       "  0.1507314145565033,\n",
       "  0.1635562777519226,\n",
       "  0.19394665956497192,\n",
       "  0.1942146122455597,\n",
       "  0.17938511073589325,\n",
       "  0.1451740562915802,\n",
       "  0.2089986801147461,\n",
       "  0.14533089101314545,\n",
       "  0.21160788834095,\n",
       "  0.1603197604417801,\n",
       "  0.10462799668312073,\n",
       "  0.15912291407585144,\n",
       "  0.2313462793827057,\n",
       "  0.1400887370109558,\n",
       "  0.15393419563770294,\n",
       "  0.15061120688915253,\n",
       "  0.15216776728630066,\n",
       "  0.16944193840026855,\n",
       "  0.18868088722229004,\n",
       "  0.17093917727470398,\n",
       "  0.23932576179504395,\n",
       "  0.37244245409965515,\n",
       "  0.22999708354473114,\n",
       "  0.3470008373260498,\n",
       "  0.21192392706871033,\n",
       "  0.26646000146865845,\n",
       "  0.2829897999763489,\n",
       "  0.20898282527923584,\n",
       "  0.25637102127075195,\n",
       "  0.19908982515335083,\n",
       "  0.27919334173202515,\n",
       "  0.30189627408981323,\n",
       "  0.2956276535987854,\n",
       "  0.15049055218696594,\n",
       "  0.2087160050868988,\n",
       "  0.21780546009540558,\n",
       "  0.18453632295131683,\n",
       "  0.1641387939453125,\n",
       "  0.1748674064874649,\n",
       "  0.2300793081521988,\n",
       "  0.22237330675125122,\n",
       "  0.22983863949775696,\n",
       "  0.21942219138145447,\n",
       "  0.19343072175979614,\n",
       "  0.19023239612579346,\n",
       "  0.16846658289432526,\n",
       "  0.2111814171075821,\n",
       "  0.33272603154182434,\n",
       "  0.2984790802001953,\n",
       "  0.26855742931365967,\n",
       "  0.18450134992599487,\n",
       "  0.2746404707431793,\n",
       "  0.19263696670532227,\n",
       "  0.24497243762016296,\n",
       "  0.20045194029808044,\n",
       "  0.3152915835380554,\n",
       "  0.24042120575904846,\n",
       "  0.1362367868423462,\n",
       "  0.18638043105602264,\n",
       "  0.19073112308979034,\n",
       "  0.1007387787103653,\n",
       "  0.13452817499637604,\n",
       "  0.22483059763908386,\n",
       "  0.08436791598796844,\n",
       "  0.3209809362888336,\n",
       "  0.1086684986948967,\n",
       "  0.11727389693260193,\n",
       "  0.1008467972278595,\n",
       "  0.42404159903526306,\n",
       "  0.16191530227661133,\n",
       "  0.39374327659606934,\n",
       "  0.13333651423454285,\n",
       "  0.04052506759762764,\n",
       "  0.07199130952358246,\n",
       "  0.17728589475154877,\n",
       "  0.1338966190814972,\n",
       "  0.22472141683101654,\n",
       "  0.14044293761253357,\n",
       "  0.6226518750190735,\n",
       "  0.2576037049293518,\n",
       "  0.036657121032476425,\n",
       "  0.24271777272224426,\n",
       "  0.2079186737537384,\n",
       "  0.17391720414161682,\n",
       "  0.06501241028308868,\n",
       "  0.07366873323917389,\n",
       "  0.08988028019666672,\n",
       "  0.07747257500886917,\n",
       "  0.11488968133926392,\n",
       "  0.06606314331293106,\n",
       "  0.06434741616249084,\n",
       "  0.0872102677822113,\n",
       "  0.060167212039232254,\n",
       "  0.08652536571025848,\n",
       "  0.11838574707508087,\n",
       "  0.09059997648000717,\n",
       "  0.0823991596698761,\n",
       "  0.19348686933517456,\n",
       "  0.09167768061161041,\n",
       "  0.1467439830303192,\n",
       "  0.13520026206970215,\n",
       "  0.14831656217575073,\n",
       "  0.06978560984134674,\n",
       "  0.087728351354599,\n",
       "  0.09198091924190521,\n",
       "  0.1241595521569252,\n",
       "  0.11179420351982117,\n",
       "  0.08643649518489838,\n",
       "  0.07318657636642456,\n",
       "  0.052843641489744186,\n",
       "  0.08407357335090637,\n",
       "  0.0553528368473053,\n",
       "  0.11209706962108612,\n",
       "  0.08764626830816269,\n",
       "  0.06538339704275131,\n",
       "  0.1120608001947403,\n",
       "  2.096780776977539,\n",
       "  1.4008396863937378,\n",
       "  2.302142381668091,\n",
       "  1.579160213470459,\n",
       "  2.3293404579162598,\n",
       "  2.5925564765930176,\n",
       "  1.6675374507904053,\n",
       "  2.0060245990753174,\n",
       "  3.08119535446167,\n",
       "  1.7052860260009766,\n",
       "  3.0102286338806152,\n",
       "  2.34649658203125,\n",
       "  1.916304349899292,\n",
       "  1.677367925643921,\n",
       "  3.118100643157959,\n",
       "  2.1902341842651367,\n",
       "  1.604812502861023,\n",
       "  1.412949562072754,\n",
       "  1.1683290004730225,\n",
       "  1.7385005950927734,\n",
       "  1.2917412519454956,\n",
       "  2.4506328105926514,\n",
       "  2.6373672485351562,\n",
       "  2.9412498474121094,\n",
       "  1.696876049041748,\n",
       "  2.5922470092773438,\n",
       "  2.5200605392456055,\n",
       "  1.1079702377319336,\n",
       "  1.7856565713882446,\n",
       "  2.418999671936035,\n",
       "  1.6374064683914185,\n",
       "  2.5217654705047607],\n",
       " 'valid_acc_avg': [0.95947265625,\n",
       "  0.91943359375,\n",
       "  0.95166015625,\n",
       "  0.97021484375,\n",
       "  0.6650390625],\n",
       " 'valid_loss_avg': [0.1796806906349957,\n",
       "  0.23799325851723552,\n",
       "  0.19821013626642525,\n",
       "  0.09395551937632263,\n",
       "  2.0795675180852413]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "n_shot = 5\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "# data set up\n",
    "train_sampler = CL_dff_loader(data_df=train_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "valid_sampler = CL_dff_loader(data_df=valid_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "# model set up\n",
    "cnn_lstm = Base_LSTM_CNN(emb_dim=768, seq_len=100, lstm_units=200, num_filters=30, kernel_sizes=[1, 3, 5], num_classes=2)\n",
    "prto_model = Protonet(cnn_lstm)\n",
    "\n",
    "train_proto(the_model=prto_model,\n",
    "            n_epochs=n_epochs,\n",
    "            learning_rate=lr,\n",
    "            train_loader_generator=train_sampler,\n",
    "            valid_loader_generator=valid_sampler,\n",
    "            n_eposides_train=32,\n",
    "            n_eposides_valid=32,\n",
    "            n_shot=n_shot,\n",
    "            estimate_theta_subsample_size=1000,\n",
    "            the_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch: 1 / 5==========\n",
      "\n",
      "Train acc: 0.8115234375, Train loss: 1.3022778192535043\n",
      "\n",
      "Validation acc: 0.92919921875, Validation loss: 0.21208304772153497\n",
      "\n",
      "Current Theta: 5.093281250000018\n",
      "\n",
      "Current Train Dataset Length: 33867\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 2 / 5==========\n",
      "\n",
      "Train acc: 0.87060546875, Train loss: 0.31803215853869915\n",
      "\n",
      "Validation acc: 0.97802734375, Validation loss: 0.079365207842784\n",
      "\n",
      "Current Theta: 6.077578125000022\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 3 / 5==========\n",
      "\n",
      "Train acc: 0.896484375, Train loss: 0.2588753621093929\n",
      "\n",
      "Validation acc: 0.89306640625, Validation loss: 0.4110972487833351\n",
      "\n",
      "Current Theta: 6.032187500000022\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 4 / 5==========\n",
      "\n",
      "Train acc: 0.89111328125, Train loss: 0.30449875397607684\n",
      "\n",
      "Validation acc: 0.9658203125, Validation loss: 0.12363101518712938\n",
      "\n",
      "Current Theta: 6.732890625000023\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 5 / 5==========\n",
      "\n",
      "Train acc: 0.91259765625, Train loss: 0.23345255386084318\n",
      "\n",
      "Validation acc: 0.93896484375, Validation loss: 0.17208711302373558\n",
      "\n",
      "Current Theta: 6.784453125000024\n",
      "\n",
      "Current Train Dataset Length: 34995\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.578125,\n",
       "  0.84375,\n",
       "  0.71875,\n",
       "  0.703125,\n",
       "  0.953125,\n",
       "  0.78125,\n",
       "  0.765625,\n",
       "  0.765625,\n",
       "  0.84375,\n",
       "  0.734375,\n",
       "  0.609375,\n",
       "  0.703125,\n",
       "  0.765625,\n",
       "  0.84375,\n",
       "  0.734375,\n",
       "  0.796875,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.828125,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.8125,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.78125,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.859375,\n",
       "  0.796875,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.859375,\n",
       "  0.734375,\n",
       "  0.8125,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.84375,\n",
       "  0.8125,\n",
       "  0.703125,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.90625,\n",
       "  0.9375,\n",
       "  0.84375,\n",
       "  0.84375,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.90625,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  1.0,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.90625,\n",
       "  0.734375,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.859375,\n",
       "  0.84375,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.84375,\n",
       "  0.8125,\n",
       "  0.921875,\n",
       "  0.84375,\n",
       "  0.921875,\n",
       "  0.75,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.859375,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.84375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.984375],\n",
       " 'train_loss': [0.6754512786865234,\n",
       "  4.816842079162598,\n",
       "  2.2192931175231934,\n",
       "  13.343446731567383,\n",
       "  1.5046991109848022,\n",
       "  5.937767028808594,\n",
       "  2.5794005393981934,\n",
       "  1.0070500373840332,\n",
       "  0.4863177537918091,\n",
       "  0.5868999361991882,\n",
       "  0.5572208166122437,\n",
       "  0.47428977489471436,\n",
       "  0.4656190574169159,\n",
       "  0.3517627716064453,\n",
       "  0.5597050189971924,\n",
       "  0.4650302827358246,\n",
       "  0.30748867988586426,\n",
       "  0.25452369451522827,\n",
       "  0.3067969083786011,\n",
       "  0.3834064304828644,\n",
       "  0.2369537651538849,\n",
       "  0.5430843234062195,\n",
       "  0.5178197622299194,\n",
       "  0.4687976837158203,\n",
       "  0.2904399335384369,\n",
       "  0.1564253270626068,\n",
       "  0.3998207449913025,\n",
       "  0.6221393942832947,\n",
       "  0.2513154149055481,\n",
       "  0.3542421758174896,\n",
       "  0.29601144790649414,\n",
       "  0.2528291940689087,\n",
       "  0.31133338809013367,\n",
       "  0.46779438853263855,\n",
       "  0.24939581751823425,\n",
       "  0.25143635272979736,\n",
       "  0.3068053722381592,\n",
       "  0.47870421409606934,\n",
       "  0.335155725479126,\n",
       "  0.21312075853347778,\n",
       "  0.3612496256828308,\n",
       "  0.21198464930057526,\n",
       "  0.26644402742385864,\n",
       "  0.3432094156742096,\n",
       "  0.2261984795331955,\n",
       "  0.2579745948314667,\n",
       "  0.2673332691192627,\n",
       "  0.28295668959617615,\n",
       "  0.3613767623901367,\n",
       "  0.33914119005203247,\n",
       "  0.523578405380249,\n",
       "  0.11670759320259094,\n",
       "  0.22326834499835968,\n",
       "  0.19280514121055603,\n",
       "  0.3352543115615845,\n",
       "  0.28468436002731323,\n",
       "  0.3666922450065613,\n",
       "  0.3667118549346924,\n",
       "  0.31025922298431396,\n",
       "  0.3237294852733612,\n",
       "  0.49527835845947266,\n",
       "  0.31221267580986023,\n",
       "  0.5583264827728271,\n",
       "  0.23590587079524994,\n",
       "  0.21783879399299622,\n",
       "  0.32580482959747314,\n",
       "  0.3409140706062317,\n",
       "  0.25275683403015137,\n",
       "  0.22772417962551117,\n",
       "  0.24567249417304993,\n",
       "  0.28885912895202637,\n",
       "  0.3339816927909851,\n",
       "  0.2724570631980896,\n",
       "  0.3027592599391937,\n",
       "  0.25359219312667847,\n",
       "  0.1944366693496704,\n",
       "  0.21547278761863708,\n",
       "  0.28977346420288086,\n",
       "  0.31437334418296814,\n",
       "  0.4032704532146454,\n",
       "  0.302554726600647,\n",
       "  0.23573756217956543,\n",
       "  0.2539672255516052,\n",
       "  0.24001628160476685,\n",
       "  0.17303068935871124,\n",
       "  0.26802054047584534,\n",
       "  0.28226029872894287,\n",
       "  0.193441241979599,\n",
       "  0.4150918126106262,\n",
       "  0.2898075580596924,\n",
       "  0.25565364956855774,\n",
       "  0.19628793001174927,\n",
       "  0.1258322149515152,\n",
       "  0.18512997031211853,\n",
       "  0.17342603206634521,\n",
       "  0.21406659483909607,\n",
       "  0.30025196075439453,\n",
       "  0.7856698632240295,\n",
       "  0.24888457357883453,\n",
       "  0.16017387807369232,\n",
       "  0.26634183526039124,\n",
       "  0.45707011222839355,\n",
       "  0.7139962315559387,\n",
       "  0.43769484758377075,\n",
       "  0.2501125633716583,\n",
       "  0.18235382437705994,\n",
       "  0.29579898715019226,\n",
       "  0.26158300042152405,\n",
       "  0.2621496021747589,\n",
       "  0.24958181381225586,\n",
       "  0.3549506664276123,\n",
       "  0.3077785074710846,\n",
       "  0.32807934284210205,\n",
       "  0.30226683616638184,\n",
       "  0.26305532455444336,\n",
       "  0.19795221090316772,\n",
       "  0.31809717416763306,\n",
       "  0.49745893478393555,\n",
       "  0.24216696619987488,\n",
       "  0.23773421347141266,\n",
       "  0.22706015408039093,\n",
       "  0.2678402066230774,\n",
       "  0.19169314205646515,\n",
       "  0.3192102313041687,\n",
       "  0.15834742784500122,\n",
       "  0.21069014072418213,\n",
       "  0.12377417087554932,\n",
       "  0.32414138317108154,\n",
       "  0.5652168989181519,\n",
       "  0.24458493292331696,\n",
       "  0.30846428871154785,\n",
       "  0.27285677194595337,\n",
       "  0.34128379821777344,\n",
       "  0.23335158824920654,\n",
       "  0.21134699881076813,\n",
       "  0.1541692167520523,\n",
       "  0.16856983304023743,\n",
       "  0.20831626653671265,\n",
       "  0.17249703407287598,\n",
       "  0.37085026502609253,\n",
       "  0.15239746868610382,\n",
       "  0.12625561654567719,\n",
       "  0.3244847059249878,\n",
       "  0.17101863026618958,\n",
       "  0.1376056671142578,\n",
       "  0.3212319314479828,\n",
       "  0.2548302114009857,\n",
       "  0.16939623653888702,\n",
       "  0.16070939600467682,\n",
       "  0.22971117496490479,\n",
       "  0.2799100875854492,\n",
       "  0.23722565174102783,\n",
       "  0.21574100852012634,\n",
       "  0.37676090002059937,\n",
       "  0.16710741817951202,\n",
       "  0.09452781826257706,\n",
       "  0.23675698041915894,\n",
       "  0.2888423800468445,\n",
       "  0.16356797516345978,\n",
       "  0.11089257150888443],\n",
       " 'train_acc_avg': [0.8115234375,\n",
       "  0.87060546875,\n",
       "  0.896484375,\n",
       "  0.89111328125,\n",
       "  0.91259765625],\n",
       " 'train_loss_avg': [1.3022778192535043,\n",
       "  0.31803215853869915,\n",
       "  0.2588753621093929,\n",
       "  0.30449875397607684,\n",
       "  0.23345255386084318],\n",
       " 'valid_acc': [0.9375,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  0.890625,\n",
       "  0.828125,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.828125,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.859375,\n",
       "  0.84375,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.859375,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.84375,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.859375,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  1.0,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.8125,\n",
       "  0.96875,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.875,\n",
       "  0.921875],\n",
       " 'valid_loss': [0.1898210197687149,\n",
       "  0.16313332319259644,\n",
       "  0.14758282899856567,\n",
       "  0.3147512674331665,\n",
       "  0.1836390644311905,\n",
       "  0.13717302680015564,\n",
       "  0.18118369579315186,\n",
       "  0.20558828115463257,\n",
       "  0.34035640954971313,\n",
       "  0.24948924779891968,\n",
       "  0.34404459595680237,\n",
       "  0.20067407190799713,\n",
       "  0.15801531076431274,\n",
       "  0.1788916140794754,\n",
       "  0.07967774569988251,\n",
       "  0.21271741390228271,\n",
       "  0.21202276647090912,\n",
       "  0.23979036509990692,\n",
       "  0.19188395142555237,\n",
       "  0.21999377012252808,\n",
       "  0.15179604291915894,\n",
       "  0.1675814688205719,\n",
       "  0.1908411681652069,\n",
       "  0.2514088749885559,\n",
       "  0.10240504145622253,\n",
       "  0.21734941005706787,\n",
       "  0.30006980895996094,\n",
       "  0.2572493553161621,\n",
       "  0.28608620166778564,\n",
       "  0.23270699381828308,\n",
       "  0.23987185955047607,\n",
       "  0.23886153101921082,\n",
       "  0.030098389834165573,\n",
       "  0.12449232488870621,\n",
       "  0.03221854567527771,\n",
       "  0.10400390625,\n",
       "  0.1476995348930359,\n",
       "  0.15143141150474548,\n",
       "  0.023934245109558105,\n",
       "  0.035206861793994904,\n",
       "  0.04987797141075134,\n",
       "  0.028053369373083115,\n",
       "  0.02549365907907486,\n",
       "  0.1781921684741974,\n",
       "  0.022632891312241554,\n",
       "  0.08784637600183487,\n",
       "  0.13655860722064972,\n",
       "  0.03549451753497124,\n",
       "  0.03973732143640518,\n",
       "  0.05517054349184036,\n",
       "  0.14397752285003662,\n",
       "  0.020534362643957138,\n",
       "  0.0736701712012291,\n",
       "  0.1686558872461319,\n",
       "  0.05406714975833893,\n",
       "  0.04105530306696892,\n",
       "  0.08002263307571411,\n",
       "  0.11912649869918823,\n",
       "  0.015201318077743053,\n",
       "  0.09049122780561447,\n",
       "  0.21597261726856232,\n",
       "  0.118785560131073,\n",
       "  0.05036354809999466,\n",
       "  0.039620205760002136,\n",
       "  0.6244536638259888,\n",
       "  0.6026861667633057,\n",
       "  0.4149135947227478,\n",
       "  0.4666018486022949,\n",
       "  0.18536365032196045,\n",
       "  0.6502069234848022,\n",
       "  0.5493874549865723,\n",
       "  0.27745410799980164,\n",
       "  0.46442312002182007,\n",
       "  0.44849497079849243,\n",
       "  0.28013837337493896,\n",
       "  0.33528587222099304,\n",
       "  0.4887072443962097,\n",
       "  0.5737818479537964,\n",
       "  0.5189059972763062,\n",
       "  0.32925790548324585,\n",
       "  0.11165764182806015,\n",
       "  0.5166043043136597,\n",
       "  0.6451194286346436,\n",
       "  0.2261754274368286,\n",
       "  0.3445087969303131,\n",
       "  0.1845833659172058,\n",
       "  0.34603869915008545,\n",
       "  0.2179029881954193,\n",
       "  0.31042975187301636,\n",
       "  0.5320776700973511,\n",
       "  0.08401608467102051,\n",
       "  0.30729708075523376,\n",
       "  0.5558345913887024,\n",
       "  0.36113882064819336,\n",
       "  0.7053236365318298,\n",
       "  0.49634093046188354,\n",
       "  0.06136593967676163,\n",
       "  0.1655251681804657,\n",
       "  0.09524112194776535,\n",
       "  0.07310191541910172,\n",
       "  0.1580226719379425,\n",
       "  0.08474855870008469,\n",
       "  0.21870437264442444,\n",
       "  0.02022010087966919,\n",
       "  0.17530643939971924,\n",
       "  0.06770298629999161,\n",
       "  0.16125747561454773,\n",
       "  0.06177236884832382,\n",
       "  0.1143653616309166,\n",
       "  0.07806451618671417,\n",
       "  0.1920740306377411,\n",
       "  0.21013875305652618,\n",
       "  0.09122984856367111,\n",
       "  0.1701190173625946,\n",
       "  0.025784730911254883,\n",
       "  0.17842000722885132,\n",
       "  0.1571129858493805,\n",
       "  0.30712538957595825,\n",
       "  0.07456359267234802,\n",
       "  0.19035738706588745,\n",
       "  0.07352433353662491,\n",
       "  0.07819467037916183,\n",
       "  0.15027974545955658,\n",
       "  0.16671276092529297,\n",
       "  0.0850537121295929,\n",
       "  0.111599400639534,\n",
       "  0.12224055826663971,\n",
       "  0.03626256436109543,\n",
       "  0.1595025211572647,\n",
       "  0.20024651288986206,\n",
       "  0.0999208614230156,\n",
       "  0.1037924513220787,\n",
       "  0.3156029284000397,\n",
       "  0.17411932349205017,\n",
       "  0.25634700059890747,\n",
       "  0.19323188066482544,\n",
       "  0.249716654419899,\n",
       "  0.19616293907165527,\n",
       "  0.08896614611148834,\n",
       "  0.18349310755729675,\n",
       "  0.15520089864730835,\n",
       "  0.06084264814853668,\n",
       "  0.1440199613571167,\n",
       "  0.05497962608933449,\n",
       "  0.18218335509300232,\n",
       "  0.2801494598388672,\n",
       "  0.17524701356887817,\n",
       "  0.45104116201400757,\n",
       "  0.1676091104745865,\n",
       "  0.11187249422073364,\n",
       "  0.08968139439821243,\n",
       "  0.14382407069206238,\n",
       "  0.09005076438188553,\n",
       "  0.2321242392063141,\n",
       "  0.15437299013137817,\n",
       "  0.11180369555950165,\n",
       "  0.14243625104427338,\n",
       "  0.0859849601984024,\n",
       "  0.31222981214523315,\n",
       "  0.1400313824415207],\n",
       " 'valid_acc_avg': [0.92919921875,\n",
       "  0.97802734375,\n",
       "  0.89306640625,\n",
       "  0.9658203125,\n",
       "  0.93896484375],\n",
       " 'valid_loss_avg': [0.21208304772153497,\n",
       "  0.079365207842784,\n",
       "  0.4110972487833351,\n",
       "  0.12363101518712938,\n",
       "  0.17208711302373558]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "n_shot = 5\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "# data set up\n",
    "train_sampler = CL_dff_loader(data_df=train_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "valid_sampler = CL_dff_loader(data_df=valid_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "# model set up\n",
    "cnn_lstm = Base_LSTM_CNN(emb_dim=768, seq_len=100, lstm_units=200, num_filters=30, kernel_sizes=[1, 3, 5], num_classes=2)\n",
    "prto_model = Protonet(cnn_lstm)\n",
    "\n",
    "train_proto(the_model=prto_model,\n",
    "            n_epochs=n_epochs,\n",
    "            learning_rate=lr,\n",
    "            train_loader_generator=train_sampler,\n",
    "            valid_loader_generator=valid_sampler,\n",
    "            n_eposides_train=32,\n",
    "            n_eposides_valid=32,\n",
    "            n_shot=n_shot,\n",
    "            estimate_theta_subsample_size=500,\n",
    "            the_device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Function Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 1.8158593750000067\n",
      "Accuracy: 0.8, Theta: 1.841406250000007\n",
      "Accuracy: 0.8, Theta: 2.019531250000008\n",
      "Accuracy: 0.8, Theta: 1.832812500000007\n",
      "Accuracy: 0.8, Theta: 1.9978125000000078\n",
      "Accuracy: 0.8, Theta: 2.2521093750000087\n",
      "Accuracy: 0.8, Theta: 1.4890625000000055\n",
      "Accuracy: 0.8, Theta: 2.3806250000000087\n",
      "Accuracy: 0.8, Theta: 1.7240625000000065\n",
      "Accuracy: 0.8, Theta: 1.7513281250000068\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 8 + [0] * 2)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 4.390312500000016\n",
      "Accuracy: 0.8, Theta: 4.125312500000014\n",
      "Accuracy: 0.8, Theta: 5.033359375000018\n",
      "Accuracy: 0.8, Theta: 4.221328125000015\n",
      "Accuracy: 0.8, Theta: 4.376484375000015\n",
      "Accuracy: 0.8, Theta: 5.094296875000017\n",
      "Accuracy: 0.8, Theta: 4.949687500000017\n",
      "Accuracy: 0.8, Theta: 5.4034375000000185\n",
      "Accuracy: 0.8, Theta: 5.36851562500002\n",
      "Accuracy: 0.8, Theta: 4.972265625000018\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 80 + [0] * 20)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 8.317031250000028\n",
      "Accuracy: 0.8, Theta: 8.398906250000028\n",
      "Accuracy: 0.8, Theta: 8.76359375000003\n",
      "Accuracy: 0.8, Theta: 8.44718750000003\n",
      "Accuracy: 0.8, Theta: 8.628984375000032\n",
      "Accuracy: 0.8, Theta: 8.674765625000033\n",
      "Accuracy: 0.8, Theta: 8.899375000000031\n",
      "Accuracy: 0.8, Theta: 8.45976562500003\n",
      "Accuracy: 0.8, Theta: 8.667890625000032\n",
      "Accuracy: 0.8, Theta: 8.324140625000028\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 800 + [0] * 200)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 12.240078125000043\n",
      "Accuracy: 0.8, Theta: 12.188515625000042\n",
      "Accuracy: 0.8, Theta: 12.460078125000043\n",
      "Accuracy: 0.8, Theta: 11.96085937500004\n",
      "Accuracy: 0.8, Theta: 12.738203125000046\n",
      "Accuracy: 0.8, Theta: 12.968359375000047\n",
      "Accuracy: 0.8, Theta: 12.535468750000046\n",
      "Accuracy: 0.8, Theta: 12.936250000000044\n",
      "Accuracy: 0.8, Theta: 12.278593750000045\n",
      "Accuracy: 0.8, Theta: 12.155625000000043\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 8000 + [0] * 2000)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
