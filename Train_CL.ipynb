{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train CL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dependencies & Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from py_irt import scoring\n",
    "from torchinfo import summary\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sortedcontainers import SortedDict\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\aug_data\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\total_data\\task\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\total_data\\fitted_IRT\n",
      "c:\\Users\\hhl08\\Desktop\\CL-crowd-training\\total_data\\merge_for_training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['merged_eval', 'merged_train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'aug_data')\n",
    "task_path = os.path.join(cwd, 'total_data', 'task')\n",
    "difficulties_path = os.path.join(cwd, 'total_data', 'fitted_IRT')\n",
    "merge_for_training_path = os.path.join(cwd, 'total_data', 'merge_for_training')\n",
    "\n",
    "if not os.path.isdir(merge_for_training_path):\n",
    "    os.mkdir(merge_for_training_path)\n",
    "    os.mkdir(os.path.join(merge_for_training_path, 'merged_train'))\n",
    "    os.mkdir(os.path.join(merge_for_training_path, 'merged_eval'))\n",
    "\n",
    "print(cwd)\n",
    "print(data_path)\n",
    "print(task_path)\n",
    "print(difficulties_path)\n",
    "print(merge_for_training_path)\n",
    "os.listdir(merge_for_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x286028f0410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4747\n",
    "\n",
    "# numpy\n",
    "np.random.seed(seed)\n",
    "# random\n",
    "random.seed(seed)\n",
    "# pytorch\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ability', 'diff', 'irt_model', 'item_ids', 'subject_ids'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class = \"Perspective\"\n",
    "\n",
    "file_va = json.load(open(os.path.join(difficulties_path, train_class, 'best_parameters.json')))\n",
    "file_va.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.231886386871338,\n",
       " -1.7343086004257202,\n",
       " -1.8631240129470825,\n",
       " -1.6610524654388428,\n",
       " 1.545536756515503]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_va['diff'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '7db0d3a1-b8e4-4ff6-96e3-cc55d9cb77a3'),\n",
       " ('1', '2b8e660a-ebfc-4c18-a3d3-295fed1dc8f4'),\n",
       " ('2', '6317c474-d3fd-418d-9fa0-217506a75f56'),\n",
       " ('3', '510ff0b4-8773-41d1-9182-e1078f6336b4'),\n",
       " ('4', 'd5eafd31-3a7c-4746-9329-1639ee8aee14')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(file_va['item_ids'].items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7db0d3a1-b8e4-4ff6-96e3-cc55d9cb77a3</td>\n",
       "      <td>4.231886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b8e660a-ebfc-4c18-a3d3-295fed1dc8f4</td>\n",
       "      <td>-1.734309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6317c474-d3fd-418d-9fa0-217506a75f56</td>\n",
       "      <td>-1.863124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510ff0b4-8773-41d1-9182-e1078f6336b4</td>\n",
       "      <td>-1.661052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d5eafd31-3a7c-4746-9329-1639ee8aee14</td>\n",
       "      <td>1.545537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID      diff\n",
       "0  7db0d3a1-b8e4-4ff6-96e3-cc55d9cb77a3  4.231886\n",
       "1  2b8e660a-ebfc-4c18-a3d3-295fed1dc8f4 -1.734309\n",
       "2  6317c474-d3fd-418d-9fa0-217506a75f56 -1.863124\n",
       "3  510ff0b4-8773-41d1-9182-e1078f6336b4 -1.661052\n",
       "4  d5eafd31-3a7c-4746-9329-1639ee8aee14  1.545537"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [file_va['item_ids'][i] for i in file_va['item_ids']]\n",
    "diff_data = pd.DataFrame({'ID': ids, 'diff': file_va['diff']})\n",
    "diff_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Merge all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Country\n",
      "Train size: 34996\n",
      "Eval  size: 14983\n",
      "Merging Intervention\n",
      "Train size: 34997\n",
      "Eval  size: 14985\n",
      "Merging Perspective\n",
      "Train size: 34995\n",
      "Eval  size: 14981\n",
      "Merging Population\n",
      "Train size: 34998\n",
      "Eval  size: 14982\n",
      "Merging Sample Size\n",
      "Train size: 34993\n",
      "Eval  size: 14980\n",
      "Merging Study Period\n",
      "Train size: 34997\n",
      "Eval  size: 14988\n"
     ]
    }
   ],
   "source": [
    "labels = ['Country','Intervention','Perspective','Population','Sample Size','Study Period']\n",
    "IRT_train = {}\n",
    "IRT_eval = {}\n",
    "\n",
    "for train_class in labels:\n",
    "    print(f'Merging {train_class}')\n",
    "    # load task & diff\n",
    "    file_va = json.load(open(os.path.join(difficulties_path, train_class, 'best_parameters.json')))\n",
    "    ids = [file_va['item_ids'][i] for i in file_va['item_ids']]\n",
    "    diff_data = pd.DataFrame({'ID': ids, 'diff': file_va['diff']})\n",
    "    task_data = pd.read_csv(os.path.join(task_path, train_class + '.csv'), index_col=0)\n",
    "    # merge train\n",
    "    train_df = pd.merge(task_data[task_data.Train==True], diff_data, on='ID')\n",
    "    train_df['Pos_support_locs'] = train_df['Pos_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    train_df['Neg_support_locs'] = train_df['Neg_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    IRT_train[train_class] = train_df[[\"ID\",\"Pos_support_locs\",\"Neg_support_locs\", \"Query_loc\",\"Label\",\"Alpha\",\"Aug_type\",\"diff\"]]\n",
    "    print(f'Train size: {len(train_df)}')\n",
    "    # train_df['diff'].hist(bins =50).plot()\n",
    "    # plt.title(train_class + \" - Train\")\n",
    "    # plt.show()\n",
    "    # merge eval\n",
    "    eval_df = pd.merge(task_data[task_data.Train==False], diff_data, on='ID')\n",
    "    eval_df['Pos_support_locs'] = eval_df['Pos_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    eval_df['Neg_support_locs'] = eval_df['Neg_support_locs'].apply(lambda x: ast.literal_eval(x))\n",
    "    IRT_eval[train_class] = eval_df[[\"ID\",\"Pos_support_locs\",\"Neg_support_locs\", \"Query_loc\",\"Label\",\"Alpha\",\"Aug_type\",\"diff\"]]\n",
    "\n",
    "    print(f'Eval  size: {len(eval_df)}')\n",
    "    # eval_df['diff'].hist(bins =50).plot()\n",
    "    # plt.title(train_class + \" - Evaluation\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005edd-1296-4630-9789-b5a5a23972f1</td>\n",
       "      <td>[6, 225, 181, 0, 211]</td>\n",
       "      <td>[596, 207, 117, 322, 249]</td>\n",
       "      <td>417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.905746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001c7fa-751f-478a-9e04-bfc836a0a514</td>\n",
       "      <td>[4, 66, 181, 179, 11]</td>\n",
       "      <td>[111, 177, 634, 508, 630]</td>\n",
       "      <td>292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.863938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00036e5c-f52d-4c1c-a88e-bae07525c43a</td>\n",
       "      <td>[34, 31, 35, 211, 58]</td>\n",
       "      <td>[443, 488, 583, 241, 268]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061dae-95a0-4afe-aee0-fb4baf6be08f</td>\n",
       "      <td>[225, 7, 164, 39, 33]</td>\n",
       "      <td>[83, 347, 186, 424, 314]</td>\n",
       "      <td>385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.221075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008324d-24a1-4489-922d-2ddff8a7519f</td>\n",
       "      <td>[5, 3, 67, 11, 51]</td>\n",
       "      <td>[449, 318, 506, 596, 503]</td>\n",
       "      <td>651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.893198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID       Pos_support_locs  \\\n",
       "0  00005edd-1296-4630-9789-b5a5a23972f1  [6, 225, 181, 0, 211]   \n",
       "1  0001c7fa-751f-478a-9e04-bfc836a0a514  [4, 66, 181, 179, 11]   \n",
       "2  00036e5c-f52d-4c1c-a88e-bae07525c43a  [34, 31, 35, 211, 58]   \n",
       "3  00061dae-95a0-4afe-aee0-fb4baf6be08f  [225, 7, 164, 39, 33]   \n",
       "4  0008324d-24a1-4489-922d-2ddff8a7519f     [5, 3, 67, 11, 51]   \n",
       "\n",
       "            Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0  [596, 207, 117, 322, 249]        417    0.0      1         3 -1.905746  \n",
       "1  [111, 177, 634, 508, 630]        292    0.0      3         1 -1.863938  \n",
       "2  [443, 488, 583, 241, 268]          0    1.0      5         4  0.985582  \n",
       "3   [83, 347, 186, 424, 314]        385    0.0      0         4  0.221075  \n",
       "4  [449, 318, 506, 596, 503]        651    0.0      4         1 -1.893198  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRT_train['Perspective'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002b905-1318-4a7a-b182-6bb79663e886</td>\n",
       "      <td>[162, 8, 204, 179, 22]</td>\n",
       "      <td>[306, 145, 342, 331, 490]</td>\n",
       "      <td>117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003fe41-aa46-4334-985c-d3e60ee43e73</td>\n",
       "      <td>[4, 36, 57, 5, 13]</td>\n",
       "      <td>[435, 121, 472, 190, 620]</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.609673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006f5e9-e00a-41f6-81f4-484cb153a0f5</td>\n",
       "      <td>[25, 15, 28, 35, 162]</td>\n",
       "      <td>[437, 126, 583, 628, 634]</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.680577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000bb31e-883e-4a48-9c70-752ad9f23d99</td>\n",
       "      <td>[11, 58, 187, 57, 28]</td>\n",
       "      <td>[564, 220, 333, 169, 277]</td>\n",
       "      <td>212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.141161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000bb35c-7762-4af1-bdb7-e7373ffecda1</td>\n",
       "      <td>[47, 68, 216, 4, 15]</td>\n",
       "      <td>[171, 421, 155, 447, 593]</td>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.903524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID        Pos_support_locs  \\\n",
       "0  0002b905-1318-4a7a-b182-6bb79663e886  [162, 8, 204, 179, 22]   \n",
       "1  0003fe41-aa46-4334-985c-d3e60ee43e73      [4, 36, 57, 5, 13]   \n",
       "2  0006f5e9-e00a-41f6-81f4-484cb153a0f5   [25, 15, 28, 35, 162]   \n",
       "3  000bb31e-883e-4a48-9c70-752ad9f23d99   [11, 58, 187, 57, 28]   \n",
       "4  000bb35c-7762-4af1-bdb7-e7373ffecda1    [47, 68, 216, 4, 15]   \n",
       "\n",
       "            Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0  [306, 145, 342, 331, 490]        117    0.0      0         0  0.688172  \n",
       "1  [435, 121, 472, 190, 620]         73    1.0      0         0  4.609673  \n",
       "2  [437, 126, 583, 628, 634]         53    1.0      0         0 -1.680577  \n",
       "3  [564, 220, 333, 169, 277]        212    1.0      0         0 -1.141161  \n",
       "4  [171, 421, 155, 447, 593]        272    0.0      0         0 -1.903524  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRT_eval['Perspective'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "pickle.dump(IRT_train, open(os.path.join(merge_for_training_path, 'merged_train', 'irt_train.pkl'), 'wb'))\n",
    "pickle.dump(IRT_eval, open(os.path.join(merge_for_training_path, 'merged_eval', 'irt_eval.pkl'), 'wb'))\n",
    "\n",
    "# save csv\n",
    "for cur_class in labels:\n",
    "    IRT_train[cur_class].to_csv(os.path.join(merge_for_training_path, 'merged_train', cur_class + '.csv'), index=False)\n",
    "    IRT_eval[cur_class].to_csv(os.path.join(merge_for_training_path, 'merged_eval', cur_class + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Load Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 660, 100, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_token_embeddings = np.load(os.path.join(data_path, 'wordvector_masked_aug.npy'))\n",
    "aug_token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 100, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = np.load(os.path.join(data_path, 'train_wordvectors.npy'))\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, shuffle=self.shuffle).__iter__()\n",
    "\n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            return next(self.data_loader)\n",
    "        except StopIteration:\n",
    "            self.data_loader = DataLoader(self.dataset, self.batch_size, shuffle=self.shuffle).__iter__()\n",
    "            return self.next_batch()\n",
    "\n",
    "\n",
    "class CL_diff_generator(Dataset):\n",
    "    def __init__(self, wv, aug_wv, id_subset, extract_dict):\n",
    "        self.wv = wv\n",
    "        self.aug_wv = aug_wv\n",
    "        self.extract_dict = extract_dict\n",
    "        self.id_subset = id_subset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cur_id = self.id_subset[index]\n",
    "        cur_support_data, cur_query_data_x, cur_query_data_y = self.get_one_element_of_batch(cur_query_dict=self.extract_dict[cur_id])\n",
    "        cur_diff = self.extract_dict[cur_id]['diff']\n",
    "\n",
    "        return cur_support_data, cur_query_data_x, cur_query_data_y, cur_diff\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_subset)\n",
    "\n",
    "    def get_one_element_of_batch(self, cur_query_dict):\n",
    "        # extract variables\n",
    "        cur_alpha = cur_query_dict['Alpha']\n",
    "        cur_aug_type = cur_query_dict['Aug_type']\n",
    "        try:\n",
    "            cur_pos_support_locs = ast.literal_eval(cur_query_dict['Pos_support_locs'])\n",
    "        except ValueError as e:\n",
    "            if isinstance(cur_query_dict['Pos_support_locs'], list):\n",
    "                cur_pos_support_locs = cur_query_dict['Pos_support_locs']\n",
    "            else:\n",
    "                raise e\n",
    "        try:\n",
    "            cur_neg_support_locs = ast.literal_eval(cur_query_dict['Neg_support_locs'])\n",
    "        except ValueError as e:\n",
    "            if isinstance(cur_query_dict['Neg_support_locs'], list):\n",
    "                cur_neg_support_locs = cur_query_dict['Neg_support_locs']\n",
    "            else:\n",
    "                raise e\n",
    "        cur_query_loc = int(cur_query_dict['Query_loc'])\n",
    "        cur_query_label = cur_query_dict['Label']\n",
    "\n",
    "        # support\n",
    "        if cur_alpha != 0:\n",
    "            support_pos = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_pos_support_locs]\n",
    "            support_neg = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_neg_support_locs]\n",
    "            # FIXME: support_data = np.concatenate((support_pos, support_neg), axis=0)  negative comes first\n",
    "            support_data = np.concatenate((support_neg, support_pos), axis=0)\n",
    "\n",
    "            # query\n",
    "            query_data = self.aug_wv[cur_alpha - 1][cur_aug_type - 1][cur_query_loc]\n",
    "        else:\n",
    "            support_pos = self.wv[cur_pos_support_locs]\n",
    "            support_neg = self.wv[cur_neg_support_locs]\n",
    "            # support_data = np.concatenate((support_pos, support_neg), axis=0) negative comes first\n",
    "            support_data = np.concatenate((support_neg, support_pos), axis=0)\n",
    "\n",
    "            # query\n",
    "            query_data = self.wv[cur_query_loc]\n",
    "\n",
    "        return support_data, query_data, int(cur_query_label)\n",
    "\n",
    "class CL_dff_loader:\n",
    "    def __init__(self, data_df, batch_size, wv, aug_wv, n_shot):\n",
    "        # attributes\n",
    "        self.wv = wv\n",
    "        self.aug_wv = aug_wv\n",
    "        self.n_shot = n_shot\n",
    "        self.batch_size = batch_size\n",
    "        # extract info from data_df & diff look up dict\n",
    "        self.extract_dict = {}\n",
    "        self.diff_dict = {}\n",
    "        for _, cur_row in data_df.iterrows():\n",
    "            self.extract_dict[cur_row['ID']] = dict(cur_row)\n",
    "            self.diff_dict[cur_row['ID']] = cur_row['diff']\n",
    "\n",
    "    def batch_generator_with_theta(self, theta):\n",
    "        # find (diff < theta) ids\n",
    "        id_subset = []\n",
    "        for cur_key in self.diff_dict:\n",
    "            if self.diff_dict[cur_key] <= theta:\n",
    "                id_subset.append(cur_key)\n",
    "        if len(id_subset) == 0:\n",
    "            raise NoSample\n",
    "        elif len(id_subset) == 1:\n",
    "            raise OnlyOneSample\n",
    "\n",
    "        # construct dataloader\n",
    "        cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict)\n",
    "        cur_dateset_len = len(cur_dataset)\n",
    "        return InfiniteDataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True), cur_dateset_len\n",
    "\n",
    "\n",
    "    def batch_generator_without_theta(self, subsample_size=-1):  # if subsample_size == -1, use all samples & infinite dataloader | if subsample_size != -1, subsample & use finite dataloader\n",
    "        if subsample_size == -1:\n",
    "            # whole dataset\n",
    "            id_subset = list(self.diff_dict)\n",
    "            # construct dataloader\n",
    "            cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict)\n",
    "            return InfiniteDataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            # subsample\n",
    "            id_subset = list(self.diff_dict)\n",
    "            id_subset = random.sample(id_subset, subsample_size)\n",
    "            # construct dataloader\n",
    "            cur_dataset = CL_diff_generator(wv=self.wv, aug_wv=self.aug_wv, id_subset=id_subset, extract_dict=self.extract_dict)\n",
    "            return DataLoader(dataset=cur_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class NoSample(Exception):\n",
    "    def __init__(self):\n",
    "        self.message = \"No sample's diff is lower than current theta\"\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class OnlyOneSample(Exception):\n",
    "    def __init__(self):\n",
    "        self.message = \"Only one sample is found, minimum requirement: 2\"\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def data_converter(support_data, query_data_x, query_data_y, n_shot, predict_mode=False):\n",
    "    \"\"\"\n",
    "    convert the data to the fomat of Protonet class, only apply to two classes\n",
    "\n",
    "    Args:\n",
    "      support_data(numpy.array): the support data, output of Data_loader::next_batch_gen()(x[0])\n",
    "      query_data_x(numpy.array): the query data, output of Data_loader::next_batch_gen()(x[1])\n",
    "      query_data_y(numpy.array): the query data label, output of Data_loader::next_batch_gen(y[0])\n",
    "\n",
    "    Returns:\n",
    "      x_support(torch.Tensor): the support shape: (n_class, n_support, *data dimension)\n",
    "      x_query(torch.Tensor): the query set, shape: (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "      y_query(torch.longTensor): the label for query set (n_class, n_query, 1), [positive, negative]\n",
    "    \"\"\"\n",
    "\n",
    "    # support data\n",
    "    x_support_pos = torch.Tensor(support_data[:, :n_shot, :, :].reshape(-1, *support_data.shape[2:]))\n",
    "    x_support_neg = torch.Tensor(support_data[:, n_shot:, :, :].reshape(-1, *support_data.shape[2:]))\n",
    "    x_support = torch.stack([x_support_pos, x_support_neg])\n",
    "\n",
    "    # query data\n",
    "    x_query = torch.Tensor(query_data_x.reshape(2, -1, *query_data_x.shape[1:]))\n",
    "\n",
    "    # y_query\n",
    "    if predict_mode is False:\n",
    "        y_query = Variable(torch.LongTensor(query_data_y).view(2, -1, 1), requires_grad=False)\n",
    "    else:\n",
    "        y_query = None\n",
    "\n",
    "    return x_support, x_query, y_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Base_LSTM_CNN                            --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "├─LSTM: 1-2                              [32, 100, 200]            161,600\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [32, 32, 100, 1]          6,432\n",
       "│    └─Conv2d: 2-2                       [32, 32, 99, 1]           12,832\n",
       "│    └─Conv2d: 2-3                       [32, 32, 98, 1]           19,232\n",
       "├─Dropout: 1-3                           [32, 96]                  --\n",
       "==========================================================================================\n",
       "Total params: 200,096\n",
       "Trainable params: 200,096\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 638.67\n",
       "==========================================================================================\n",
       "Input size (MB): 1.28\n",
       "Forward/backward pass size (MB): 7.55\n",
       "Params size (MB): 0.80\n",
       "Estimated Total Size (MB): 9.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Base_LSTM_CNN(nn.Module):\n",
    "\n",
    "    # define all the layers used in model\n",
    "    def __init__(self, emb_dim, seq_len, lstm_units, num_filters, kernel_sizes, num_classes, dropout_rate = 0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_units = lstm_units\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
    "        self.lstm = nn.LSTM(emb_dim,\n",
    "                            lstm_units,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.num_filters, (f, 2*self.lstm_units)) for f in self.kernel_sizes])\n",
    "        # self.fc = nn.Linear(len(kernel_sizes)*self.num_filters, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # (N, seq_len, 2*lstm_units)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.size())\n",
    "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
    "        # print(x[0].size())\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
    "        x = torch.cat(x, 1)  # N, len(filter_sizes)* num_filters\n",
    "        #print(x.size())\n",
    "        x = self.dropout(x)  # N, len(filter_sizes)* num_filters\n",
    "        # logit = self.fc(x)  # (N, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Base_LSTM_CNN(100, 200, 100, 32, [1,2,3], 2)\n",
    "summary(model,(32, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Base_CNN                                 --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [32, 32, 100, 1]          24,608\n",
       "│    └─Conv2d: 2-2                       [32, 32, 98, 1]           73,760\n",
       "│    └─Conv2d: 2-3                       [32, 32, 96, 1]           122,912\n",
       "==========================================================================================\n",
       "Total params: 221,280\n",
       "Trainable params: 221,280\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 687.64\n",
       "==========================================================================================\n",
       "Input size (MB): 9.83\n",
       "Forward/backward pass size (MB): 2.41\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 13.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Base_CNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_filter, kernel_sizes):\n",
    "        # initialization\n",
    "        super(Base_CNN, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_filter = num_filter\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        # convolution layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.num_filter, (f, self.emb_dim)) for f in self.kernel_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x).squeeze(-1)) for conv in self.convs]  # output of three conv\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # continue with 3 maxpooling\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "cnn_base_model_1 = Base_CNN(emb_dim=768, num_filter=32, kernel_sizes=[1,3,5])\n",
    "summary(cnn_base_model_1, (32, 100, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "class Protonet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(Protonet, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_dist(x, y):\n",
    "        n = x.size(0)\n",
    "        m = y.size(0)\n",
    "        d = x.size(1)\n",
    "        assert d == y.size(1)\n",
    "\n",
    "        x = x.unsqueeze(1).expand(n, m, d)\n",
    "        y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "        return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "    def forward(self, x_support, x_query, y_query):\n",
    "        \"\"\"\n",
    "        Forward for prototypical network\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "          y_query(torch.longTensor): the label for query set (n_class, n_query, 1)\n",
    "\n",
    "        returns:\n",
    "          loss(torch.Tensor): negative log likelihood to be minimized, will be used to update the parameters via backprobagation\n",
    "          acc(float): accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # loss\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        loss = -log_p_y.gather(2, y_query).squeeze().view(-1).mean()  # loss\n",
    "        _, y_hat = log_p_y.max(2) # return: max, max_indicies\n",
    "        acc = torch.eq(y_hat, y_query.squeeze()).float().mean().item()\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def evaluation(self, x_support, x_query, y_query):\n",
    "        \"\"\"\n",
    "        evaluation function, will return classes, accuracy, loss(float, requires_grad=False)\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "          y_query(torch.longTensor): the label for query set (n_class, n_query, 1)\n",
    "\n",
    "        returns:\n",
    "          y_hat(torch.Tensor): the prediction label\n",
    "          loss(float): loss\n",
    "          acc(float): accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # loss\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        loss = -log_p_y.gather(2, y_query).squeeze().view(-1).mean().item()  # loss\n",
    "        _, y_hat = log_p_y.max(2) # return: max, max_indicies\n",
    "        acc = torch.eq(y_hat, y_query.squeeze()).float().mean().item()\n",
    "\n",
    "        return y_hat, acc, loss\n",
    "\n",
    "    def predict_prob(self, x_support, x_query):\n",
    "        \"\"\"\n",
    "\n",
    "        make a prediction, return positive probability\n",
    "\n",
    "        Args:\n",
    "          x_support(torch.Tensor): the support set, assume the shape (n_class, n_support, *data dimension)\n",
    "          x_query(torch.Tensor): the query set, assume the shape (n_class, n_query, *data dimension), x_query::n_class == x_support::n_class\n",
    "\n",
    "        returns:\n",
    "          p_y(float): the average probability\n",
    "        \"\"\"\n",
    "\n",
    "        # find number of class, number of query, number of support\n",
    "        n_class = x_support.size(0)\n",
    "        n_support = x_support.size(1)\n",
    "        n_query = x_query.size(1)\n",
    "\n",
    "        # concat the support and query to pass the encoder at one time\n",
    "        x = torch.cat([x_support.view(n_class * n_support, *x_support.size()[2:]), # shape (n_class * n_support, *data dimension)\n",
    "                       x_query.view(n_class * n_query, *x_query.size()[2:])],  # shape: (n_class * n_support, *data dimension)\n",
    "                      dim=0)\n",
    "        z = self.encoder.forward(x)  # pass encoder\n",
    "        z_dim = z.size(-1)  # dimension of latent vector\n",
    "\n",
    "        # estimate the prototypes\n",
    "        prototypes = z[:n_class * n_support].view(n_class, n_support, z_dim).mean(dim=1)  # take average on the different support vector(dim=1)\n",
    "\n",
    "        # extract the latent query vector\n",
    "        querys = z[n_class * n_support:]\n",
    "\n",
    "        # calculate the distance\n",
    "        dists = self.euclidean_dist(querys, prototypes)\n",
    "\n",
    "        # probability\n",
    "        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)  # log probability\n",
    "        p_y = torch.exp(log_p_y).view(-1, 2).mean(dim=0) # average probability\n",
    "\n",
    "        return p_y[0].item()\n",
    "        # return p_y[1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train_proto(the_model,\n",
    "                n_epochs,\n",
    "                n_eposides_train,\n",
    "                n_eposides_valid,\n",
    "                learning_rate,\n",
    "                train_loader_generator,\n",
    "                valid_loader_generator,\n",
    "                n_shot,\n",
    "                the_device,\n",
    "                learning_rate_schedule=False,\n",
    "                update_step=100,\n",
    "                gamma=0.5,\n",
    "                save_model=False,\n",
    "                save_path=None,\n",
    "                estimate_theta_subsample_size=1000,\n",
    "                verbose=True):\n",
    "\n",
    "    # move model\n",
    "    the_model = the_model.to(the_device)\n",
    "\n",
    "    # history\n",
    "    history = {'train_acc': [], 'train_loss': [],\n",
    "               'train_acc_avg': [], 'train_loss_avg': [],\n",
    "               'valid_acc': [], 'valid_loss': [],\n",
    "               'valid_acc_avg': [], 'valid_loss_avg': []}\n",
    "\n",
    "    # set up optimizer\n",
    "    optimizer = torch.optim.Adam(the_model.parameters(), lr=learning_rate)\n",
    "    # if schedule learning rate\n",
    "    if learning_rate_schedule is True:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                    step_size=update_step,\n",
    "                                                    gamma=gamma)\n",
    "        update_count = 0\n",
    "        if verbose:\n",
    "            print(f\"Initial learning rate: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "    # epoch loop\n",
    "    for i in range(n_epochs):\n",
    "        # 1. estimate theta\n",
    "        # data loader\n",
    "        cur_theta_estimate_dataloader = train_loader_generator.batch_generator_without_theta(subsample_size=estimate_theta_subsample_size)\n",
    "        theta_pred = []\n",
    "        theta_label = []\n",
    "        theta_diffs = []\n",
    "        # set model to eval\n",
    "        model.eval()\n",
    "        # evaluate sampled data\n",
    "        for cur_support_data, cur_query_data_x, cur_query_data_y, cur_diff in cur_theta_estimate_dataloader:\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # evaluation\n",
    "            y_hat, acc, loss = the_model.evaluation(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # metrics\n",
    "            theta_pred.extend(y_hat.view(-1).long().cpu().numpy().tolist())\n",
    "            theta_label.extend(cur_query_data_y.tolist())\n",
    "            theta_diffs.extend(cur_diff.numpy().tolist())\n",
    "        # evaluate theta\n",
    "        theta_diffs = np.array(theta_diffs)\n",
    "        theta_pred = np.array(theta_pred)\n",
    "        theta_label = np.array(theta_label)\n",
    "        theta_resp = np.where(theta_pred == theta_label, 1, 0)\n",
    "        cur_theta = scoring.calculate_theta(difficulties=theta_diffs, response_pattern=theta_resp)[0]\n",
    "\n",
    "        # 2. train\n",
    "        the_model.train()\n",
    "        train_cumulative_acc = 0\n",
    "        train_cumulative_loss = 0\n",
    "        cur_train_loader, cur_train_dataset_len = train_loader_generator.batch_generator_with_theta(theta=cur_theta)\n",
    "        for _ in range(n_eposides_train):\n",
    "            # retrieve data from train loader\n",
    "            cur_support_data, cur_query_data_x, cur_query_data_y, _ = cur_train_loader.next_batch()\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # forward\n",
    "            cur_loss, cur_acc = the_model(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # backward\n",
    "            cur_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # learning rate schedule\n",
    "            if learning_rate_schedule is True:\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "                if update_count % update_step == 0 and verbose:\n",
    "                    print(f\"Update learning rate to {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "            # cumulative loss and acc\n",
    "            train_cumulative_acc += cur_acc\n",
    "            train_cumulative_loss += cur_loss.item()\n",
    "            # record\n",
    "            history['train_acc'].append(cur_acc)\n",
    "            history['train_loss'].append(cur_loss.item())\n",
    "\n",
    "        # 3. evaluation\n",
    "        valid_cumulative_acc = 0\n",
    "        valid_cumulative_loss = 0\n",
    "        cur_valid_loader = valid_loader_generator.batch_generator_without_theta()\n",
    "        for _ in range(n_eposides_valid):\n",
    "            the_model.eval()\n",
    "            # retrieve data from validation loader\n",
    "            cur_support_data, cur_query_data_x, cur_query_data_y, cur_diff = cur_valid_loader.next_batch()\n",
    "            cur_support_data = cur_support_data.numpy()\n",
    "            cur_query_data_x = cur_query_data_x.numpy()\n",
    "            cur_query_data_y = cur_query_data_y.numpy()\n",
    "            # transform data\n",
    "            cur_x_support, cur_x_query, cur_y_query = data_converter(support_data=cur_support_data,\n",
    "                                                                     query_data_x=cur_query_data_x,\n",
    "                                                                     query_data_y=cur_query_data_y,\n",
    "                                                                     n_shot=n_shot)\n",
    "            # move data to gpu:\n",
    "            cur_x_support = cur_x_support.to(device)\n",
    "            cur_x_query = cur_x_query.to(device)\n",
    "            cur_y_query = cur_y_query.to(device)\n",
    "            # evaluation\n",
    "            y_hat, acc, loss = the_model.evaluation(cur_x_support, cur_x_query, cur_y_query)\n",
    "            # metrics\n",
    "            valid_cumulative_acc += acc\n",
    "            valid_cumulative_loss += loss\n",
    "            # record\n",
    "            history['valid_acc'].append(acc)\n",
    "            history['valid_loss'].append(loss)\n",
    "\n",
    "        # record\n",
    "        history['train_acc_avg'].append(train_cumulative_acc / n_eposides_train)\n",
    "        history['train_loss_avg'].append(train_cumulative_loss / n_eposides_train)\n",
    "        history['valid_acc_avg'].append(valid_cumulative_acc / n_eposides_valid)\n",
    "        history['valid_loss_avg'].append(valid_cumulative_loss / n_eposides_valid)\n",
    "\n",
    "        # verbose\n",
    "        if verbose:\n",
    "            print('=' * 10 + f'Epoch: {i + 1} / {n_epochs}' + '=' * 10)\n",
    "            print(f'\\nTrain acc: {train_cumulative_acc / n_eposides_train}, Train loss: {train_cumulative_loss / n_eposides_train}')\n",
    "            print(f'\\nValidation acc: {valid_cumulative_acc / n_eposides_valid}, Validation loss: {valid_cumulative_loss / n_eposides_valid}')\n",
    "            print(f'\\nCurrent Theta: {cur_theta}')\n",
    "            print(f'\\nCurrent Train Dataset Length: {cur_train_dataset_len}')\n",
    "            print('\\n')\n",
    "            print('=' * 37)\n",
    "\n",
    "    # save model\n",
    "    if save_model:\n",
    "        torch.save(the_model.state_dict(), save_path)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = IRT_train['Perspective']\n",
    "valid_df = IRT_eval[\"Perspective\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005edd-1296-4630-9789-b5a5a23972f1</td>\n",
       "      <td>[6, 225, 181, 0, 211]</td>\n",
       "      <td>[596, 207, 117, 322, 249]</td>\n",
       "      <td>417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.905746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001c7fa-751f-478a-9e04-bfc836a0a514</td>\n",
       "      <td>[4, 66, 181, 179, 11]</td>\n",
       "      <td>[111, 177, 634, 508, 630]</td>\n",
       "      <td>292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.863938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00036e5c-f52d-4c1c-a88e-bae07525c43a</td>\n",
       "      <td>[34, 31, 35, 211, 58]</td>\n",
       "      <td>[443, 488, 583, 241, 268]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.985582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00061dae-95a0-4afe-aee0-fb4baf6be08f</td>\n",
       "      <td>[225, 7, 164, 39, 33]</td>\n",
       "      <td>[83, 347, 186, 424, 314]</td>\n",
       "      <td>385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.221075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008324d-24a1-4489-922d-2ddff8a7519f</td>\n",
       "      <td>[5, 3, 67, 11, 51]</td>\n",
       "      <td>[449, 318, 506, 596, 503]</td>\n",
       "      <td>651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.893198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34990</th>\n",
       "      <td>fff78c18-c9aa-455f-883f-33c8beead374</td>\n",
       "      <td>[10, 179, 153, 11, 0]</td>\n",
       "      <td>[381, 532, 201, 259, 158]</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.591509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34991</th>\n",
       "      <td>fff955d3-eccc-4976-9e02-c5a403773555</td>\n",
       "      <td>[44, 72, 216, 204, 0]</td>\n",
       "      <td>[148, 159, 120, 263, 556]</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.311224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34992</th>\n",
       "      <td>fff975f8-ebd8-4acb-9367-408cfa1ce09b</td>\n",
       "      <td>[31, 35, 55, 0, 72]</td>\n",
       "      <td>[521, 80, 605, 301, 137]</td>\n",
       "      <td>641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.914347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34993</th>\n",
       "      <td>fffd3a56-1a65-45e4-8dc8-9430cb3f94de</td>\n",
       "      <td>[68, 187, 43, 0, 198]</td>\n",
       "      <td>[435, 146, 88, 477, 393]</td>\n",
       "      <td>564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.503651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34994</th>\n",
       "      <td>fffec17f-253c-4caa-a63f-ea9f4cc450c3</td>\n",
       "      <td>[41, 217, 11, 5, 12]</td>\n",
       "      <td>[617, 377, 298, 654, 414]</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.298037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34995 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID       Pos_support_locs  \\\n",
       "0      00005edd-1296-4630-9789-b5a5a23972f1  [6, 225, 181, 0, 211]   \n",
       "1      0001c7fa-751f-478a-9e04-bfc836a0a514  [4, 66, 181, 179, 11]   \n",
       "2      00036e5c-f52d-4c1c-a88e-bae07525c43a  [34, 31, 35, 211, 58]   \n",
       "3      00061dae-95a0-4afe-aee0-fb4baf6be08f  [225, 7, 164, 39, 33]   \n",
       "4      0008324d-24a1-4489-922d-2ddff8a7519f     [5, 3, 67, 11, 51]   \n",
       "...                                     ...                    ...   \n",
       "34990  fff78c18-c9aa-455f-883f-33c8beead374  [10, 179, 153, 11, 0]   \n",
       "34991  fff955d3-eccc-4976-9e02-c5a403773555  [44, 72, 216, 204, 0]   \n",
       "34992  fff975f8-ebd8-4acb-9367-408cfa1ce09b    [31, 35, 55, 0, 72]   \n",
       "34993  fffd3a56-1a65-45e4-8dc8-9430cb3f94de  [68, 187, 43, 0, 198]   \n",
       "34994  fffec17f-253c-4caa-a63f-ea9f4cc450c3   [41, 217, 11, 5, 12]   \n",
       "\n",
       "                Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0      [596, 207, 117, 322, 249]        417    0.0      1         3 -1.905746  \n",
       "1      [111, 177, 634, 508, 630]        292    0.0      3         1 -1.863938  \n",
       "2      [443, 488, 583, 241, 268]          0    1.0      5         4  0.985582  \n",
       "3       [83, 347, 186, 424, 314]        385    0.0      0         4  0.221075  \n",
       "4      [449, 318, 506, 596, 503]        651    0.0      4         1 -1.893198  \n",
       "...                          ...        ...    ...    ...       ...       ...  \n",
       "34990  [381, 532, 201, 259, 158]          8    1.0      3         4  1.591509  \n",
       "34991  [148, 159, 120, 263, 556]         13    1.0      0         4 -1.311224  \n",
       "34992   [521, 80, 605, 301, 137]        641    0.0      1         1 -1.914347  \n",
       "34993   [435, 146, 88, 477, 393]        564    0.0      4         2 -1.503651  \n",
       "34994  [617, 377, 298, 654, 414]          9    1.0      3         4 -1.298037  \n",
       "\n",
       "[34995 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pos_support_locs</th>\n",
       "      <th>Neg_support_locs</th>\n",
       "      <th>Query_loc</th>\n",
       "      <th>Label</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Aug_type</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002b905-1318-4a7a-b182-6bb79663e886</td>\n",
       "      <td>[162, 8, 204, 179, 22]</td>\n",
       "      <td>[306, 145, 342, 331, 490]</td>\n",
       "      <td>117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003fe41-aa46-4334-985c-d3e60ee43e73</td>\n",
       "      <td>[4, 36, 57, 5, 13]</td>\n",
       "      <td>[435, 121, 472, 190, 620]</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.609673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006f5e9-e00a-41f6-81f4-484cb153a0f5</td>\n",
       "      <td>[25, 15, 28, 35, 162]</td>\n",
       "      <td>[437, 126, 583, 628, 634]</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.680577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000bb31e-883e-4a48-9c70-752ad9f23d99</td>\n",
       "      <td>[11, 58, 187, 57, 28]</td>\n",
       "      <td>[564, 220, 333, 169, 277]</td>\n",
       "      <td>212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.141161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000bb35c-7762-4af1-bdb7-e7373ffecda1</td>\n",
       "      <td>[47, 68, 216, 4, 15]</td>\n",
       "      <td>[171, 421, 155, 447, 593]</td>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.903524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>fff38f50-782b-4ac2-b949-bcab0f116ccf</td>\n",
       "      <td>[68, 204, 34, 164, 67]</td>\n",
       "      <td>[582, 249, 511, 573, 91]</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.497381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>fff3e072-6abd-48f4-8830-ea7a7f47e862</td>\n",
       "      <td>[8, 214, 64, 3, 57]</td>\n",
       "      <td>[615, 524, 433, 104, 367]</td>\n",
       "      <td>161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.663606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>fff72ad5-39fd-41be-93be-6cb6bea91ee0</td>\n",
       "      <td>[211, 179, 27, 13, 64]</td>\n",
       "      <td>[279, 359, 246, 239, 288]</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.722035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>fffc13f0-55fb-4df9-9ff2-443bc63934be</td>\n",
       "      <td>[225, 13, 19, 167, 67]</td>\n",
       "      <td>[251, 405, 380, 580, 446]</td>\n",
       "      <td>207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>fffd5d68-50c7-4b51-a07b-08b1ecd01812</td>\n",
       "      <td>[17, 72, 40, 68, 181]</td>\n",
       "      <td>[119, 584, 419, 558, 180]</td>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.752382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14981 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ID        Pos_support_locs  \\\n",
       "0      0002b905-1318-4a7a-b182-6bb79663e886  [162, 8, 204, 179, 22]   \n",
       "1      0003fe41-aa46-4334-985c-d3e60ee43e73      [4, 36, 57, 5, 13]   \n",
       "2      0006f5e9-e00a-41f6-81f4-484cb153a0f5   [25, 15, 28, 35, 162]   \n",
       "3      000bb31e-883e-4a48-9c70-752ad9f23d99   [11, 58, 187, 57, 28]   \n",
       "4      000bb35c-7762-4af1-bdb7-e7373ffecda1    [47, 68, 216, 4, 15]   \n",
       "...                                     ...                     ...   \n",
       "14976  fff38f50-782b-4ac2-b949-bcab0f116ccf  [68, 204, 34, 164, 67]   \n",
       "14977  fff3e072-6abd-48f4-8830-ea7a7f47e862     [8, 214, 64, 3, 57]   \n",
       "14978  fff72ad5-39fd-41be-93be-6cb6bea91ee0  [211, 179, 27, 13, 64]   \n",
       "14979  fffc13f0-55fb-4df9-9ff2-443bc63934be  [225, 13, 19, 167, 67]   \n",
       "14980  fffd5d68-50c7-4b51-a07b-08b1ecd01812   [17, 72, 40, 68, 181]   \n",
       "\n",
       "                Neg_support_locs  Query_loc  Label  Alpha  Aug_type      diff  \n",
       "0      [306, 145, 342, 331, 490]        117    0.0      0         0  0.688172  \n",
       "1      [435, 121, 472, 190, 620]         73    1.0      0         0  4.609673  \n",
       "2      [437, 126, 583, 628, 634]         53    1.0      0         0 -1.680577  \n",
       "3      [564, 220, 333, 169, 277]        212    1.0      0         0 -1.141161  \n",
       "4      [171, 421, 155, 447, 593]        272    0.0      0         0 -1.903524  \n",
       "...                          ...        ...    ...    ...       ...       ...  \n",
       "14976   [582, 249, 511, 573, 91]         70    1.0      0         0  2.497381  \n",
       "14977  [615, 524, 433, 104, 367]        161    0.0      0         0 -0.663606  \n",
       "14978  [279, 359, 246, 239, 288]         37    1.0      0         0 -1.722035  \n",
       "14979  [251, 405, 380, 580, 446]        207    0.0      0         0  0.413515  \n",
       "14980  [119, 584, 419, 558, 180]         91    0.0      0         0  0.752382  \n",
       "\n",
       "[14981 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['diff'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_df['diff'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Epoch: 1 / 5==========\n",
      "\n",
      "Train acc: 0.8212890625, Train loss: 1.4849986364133656\n",
      "\n",
      "Validation acc: 0.8802083333333334, Validation loss: 0.292965163787206\n",
      "\n",
      "Current Theta: 2.89414062500001\n",
      "\n",
      "Current Train Dataset Length: 30918\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 2 / 5==========\n",
      "\n",
      "Train acc: 0.9072265625, Train loss: 0.22956230561248958\n",
      "\n",
      "Validation acc: 0.90625, Validation loss: 0.18001898129781088\n",
      "\n",
      "Current Theta: 2.9933593750000105\n",
      "\n",
      "Current Train Dataset Length: 31136\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 3 / 5==========\n",
      "\n",
      "Train acc: 0.91015625, Train loss: 0.21813169633969665\n",
      "\n",
      "Validation acc: 0.984375, Validation loss: 0.13405820727348328\n",
      "\n",
      "Current Theta: 3.5443750000000125\n",
      "\n",
      "Current Train Dataset Length: 32111\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 4 / 5==========\n",
      "\n",
      "Train acc: 0.94677734375, Train loss: 0.12195759289897978\n",
      "\n",
      "Validation acc: 0.9479166666666666, Validation loss: 0.204972542822361\n",
      "\n",
      "Current Theta: 2.5932812500000093\n",
      "\n",
      "Current Train Dataset Length: 30297\n",
      "\n",
      "\n",
      "=====================================\n",
      "==========Epoch: 5 / 5==========\n",
      "\n",
      "Train acc: 0.95703125, Train loss: 0.1045058643212542\n",
      "\n",
      "Validation acc: 0.9583333333333334, Validation loss: 0.20433307625353336\n",
      "\n",
      "Current Theta: 3.188984375000011\n",
      "\n",
      "Current Train Dataset Length: 31505\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': [0.765625,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.875,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.796875,\n",
       "  0.6875,\n",
       "  0.640625,\n",
       "  0.828125,\n",
       "  0.8125,\n",
       "  0.734375,\n",
       "  0.6875,\n",
       "  0.765625,\n",
       "  0.65625,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.8125,\n",
       "  0.859375,\n",
       "  0.84375,\n",
       "  0.859375,\n",
       "  0.796875,\n",
       "  0.8125,\n",
       "  0.84375,\n",
       "  0.828125,\n",
       "  0.84375,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.734375,\n",
       "  0.90625,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.828125,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.96875,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.984375,\n",
       "  0.859375,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.90625,\n",
       "  0.984375,\n",
       "  0.890625,\n",
       "  0.875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.859375,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.875,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.875,\n",
       "  0.90625,\n",
       "  0.84375,\n",
       "  0.875,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.875,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  1.0,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.921875,\n",
       "  0.890625,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  0.921875,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.90625,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.953125,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.984375,\n",
       "  1.0,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.953125,\n",
       "  0.984375,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125,\n",
       "  0.96875,\n",
       "  0.9375,\n",
       "  0.953125],\n",
       " 'train_loss': [0.6598913073539734,\n",
       "  0.7473009824752808,\n",
       "  3.809436082839966,\n",
       "  14.869741439819336,\n",
       "  2.490044593811035,\n",
       "  4.764532089233398,\n",
       "  5.269038200378418,\n",
       "  1.1138194799423218,\n",
       "  2.112362861633301,\n",
       "  1.0632210969924927,\n",
       "  1.9945870637893677,\n",
       "  0.7573084235191345,\n",
       "  0.8610740900039673,\n",
       "  0.5810776352882385,\n",
       "  0.7205321192741394,\n",
       "  0.5105863809585571,\n",
       "  0.49030569195747375,\n",
       "  0.3881848156452179,\n",
       "  0.34268152713775635,\n",
       "  0.35219907760620117,\n",
       "  0.3184286952018738,\n",
       "  0.3799116015434265,\n",
       "  0.37505561113357544,\n",
       "  0.31624120473861694,\n",
       "  0.35789939761161804,\n",
       "  0.3418773412704468,\n",
       "  0.21153324842453003,\n",
       "  0.30665072798728943,\n",
       "  0.2445029318332672,\n",
       "  0.12072291970252991,\n",
       "  0.1378510743379593,\n",
       "  0.5113566517829895,\n",
       "  0.19874757528305054,\n",
       "  0.2712339758872986,\n",
       "  0.16686561703681946,\n",
       "  0.280179500579834,\n",
       "  0.2576903998851776,\n",
       "  0.22536322474479675,\n",
       "  0.22355838119983673,\n",
       "  0.14074434340000153,\n",
       "  0.19750837981700897,\n",
       "  0.3086838126182556,\n",
       "  0.1820533275604248,\n",
       "  0.27083760499954224,\n",
       "  0.08454591035842896,\n",
       "  0.3035975396633148,\n",
       "  0.11052874475717545,\n",
       "  0.21068385243415833,\n",
       "  0.41168975830078125,\n",
       "  0.2086484730243683,\n",
       "  0.30613118410110474,\n",
       "  0.0819644182920456,\n",
       "  0.36655279994010925,\n",
       "  0.2337672859430313,\n",
       "  0.19255638122558594,\n",
       "  0.252548485994339,\n",
       "  0.20124894380569458,\n",
       "  0.2038002908229828,\n",
       "  0.23646773397922516,\n",
       "  0.22839145362377167,\n",
       "  0.4250509440898895,\n",
       "  0.21415311098098755,\n",
       "  0.1648891568183899,\n",
       "  0.18531116843223572,\n",
       "  0.06948225945234299,\n",
       "  0.2256171852350235,\n",
       "  0.18860769271850586,\n",
       "  0.30461984872817993,\n",
       "  0.1461777538061142,\n",
       "  0.1884728968143463,\n",
       "  0.266720175743103,\n",
       "  0.17342020571231842,\n",
       "  0.15790855884552002,\n",
       "  0.3420616388320923,\n",
       "  0.24814847111701965,\n",
       "  0.2804965078830719,\n",
       "  0.227430522441864,\n",
       "  0.22121156752109528,\n",
       "  0.2957876920700073,\n",
       "  0.45932072401046753,\n",
       "  0.21269330382347107,\n",
       "  0.2191963940858841,\n",
       "  0.20023632049560547,\n",
       "  0.2805379331111908,\n",
       "  0.25950056314468384,\n",
       "  0.20213553309440613,\n",
       "  0.16135676205158234,\n",
       "  0.11608294397592545,\n",
       "  0.40312838554382324,\n",
       "  0.23782944679260254,\n",
       "  0.13756252825260162,\n",
       "  0.13937053084373474,\n",
       "  0.2290421575307846,\n",
       "  0.07863365113735199,\n",
       "  0.15878456830978394,\n",
       "  0.14863955974578857,\n",
       "  0.11691387742757797,\n",
       "  0.15586017072200775,\n",
       "  0.1440127193927765,\n",
       "  0.13903796672821045,\n",
       "  0.12029458582401276,\n",
       "  0.13650840520858765,\n",
       "  0.022521473467350006,\n",
       "  0.13356100022792816,\n",
       "  0.041915297508239746,\n",
       "  0.2252042591571808,\n",
       "  0.07594674080610275,\n",
       "  0.1536751091480255,\n",
       "  0.11464579403400421,\n",
       "  0.12812188267707825,\n",
       "  0.10248499363660812,\n",
       "  0.1031005010008812,\n",
       "  0.27310213446617126,\n",
       "  0.04705122858285904,\n",
       "  0.22964778542518616,\n",
       "  0.1763257533311844,\n",
       "  0.07636477053165436,\n",
       "  0.11576975882053375,\n",
       "  0.09546267986297607,\n",
       "  0.06868001818656921,\n",
       "  0.12350569665431976,\n",
       "  0.0626612976193428,\n",
       "  0.15944840013980865,\n",
       "  0.08629829436540604,\n",
       "  0.13027939200401306,\n",
       "  0.07230304181575775,\n",
       "  0.21294859051704407,\n",
       "  0.058989353477954865,\n",
       "  0.11499281972646713,\n",
       "  0.10006531327962875,\n",
       "  0.06705991923809052,\n",
       "  0.20919066667556763,\n",
       "  0.04371554031968117,\n",
       "  0.07144642621278763,\n",
       "  0.11836716532707214,\n",
       "  0.11200089752674103,\n",
       "  0.05139422416687012,\n",
       "  0.1542302370071411,\n",
       "  0.12262272834777832,\n",
       "  0.1563127636909485,\n",
       "  0.1365833282470703,\n",
       "  0.1476910412311554,\n",
       "  0.08972455561161041,\n",
       "  0.0960618108510971,\n",
       "  0.1322764754295349,\n",
       "  0.06328108161687851,\n",
       "  0.04064805805683136,\n",
       "  0.04199858754873276,\n",
       "  0.10394688695669174,\n",
       "  0.03737892955541611,\n",
       "  0.1168598085641861,\n",
       "  0.023607410490512848,\n",
       "  0.14383114874362946,\n",
       "  0.11534914374351501,\n",
       "  0.08385466784238815,\n",
       "  0.10923877358436584,\n",
       "  0.04993540048599243,\n",
       "  0.07692182064056396,\n",
       "  0.13880518078804016,\n",
       "  0.2747948467731476],\n",
       " 'train_acc_avg': [0.8212890625,\n",
       "  0.9072265625,\n",
       "  0.91015625,\n",
       "  0.94677734375,\n",
       "  0.95703125],\n",
       " 'train_loss_avg': [1.4849986364133656,\n",
       "  0.22956230561248958,\n",
       "  0.21813169633969665,\n",
       "  0.12195759289897978,\n",
       "  0.1045058643212542],\n",
       " 'valid_acc': [0.921875,\n",
       "  0.828125,\n",
       "  0.890625,\n",
       "  0.90625,\n",
       "  0.890625,\n",
       "  0.921875,\n",
       "  0.96875,\n",
       "  1.0,\n",
       "  0.984375,\n",
       "  0.9375,\n",
       "  0.984375,\n",
       "  0.921875,\n",
       "  0.921875,\n",
       "  0.984375,\n",
       "  0.96875],\n",
       " 'valid_loss': [0.17821434140205383,\n",
       "  0.4675554633140564,\n",
       "  0.2331256866455078,\n",
       "  0.18104159832000732,\n",
       "  0.17412717640399933,\n",
       "  0.18488816916942596,\n",
       "  0.16295751929283142,\n",
       "  0.06984931230545044,\n",
       "  0.16936779022216797,\n",
       "  0.1011442095041275,\n",
       "  0.04316911846399307,\n",
       "  0.4706043004989624,\n",
       "  0.5518448948860168,\n",
       "  0.024121811613440514,\n",
       "  0.03703252226114273],\n",
       " 'valid_acc_avg': [0.8802083333333334,\n",
       "  0.90625,\n",
       "  0.984375,\n",
       "  0.9479166666666666,\n",
       "  0.9583333333333334],\n",
       " 'valid_loss_avg': [0.292965163787206,\n",
       "  0.18001898129781088,\n",
       "  0.13405820727348328,\n",
       "  0.204972542822361,\n",
       "  0.20433307625353336]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "n_shot = 5\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "# data set up\n",
    "train_sampler = CL_dff_loader(data_df=train_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "valid_sampler = CL_dff_loader(data_df=valid_df,\n",
    "                              batch_size=batch_size,\n",
    "                              wv=token_embeddings,\n",
    "                              aug_wv=aug_token_embeddings,\n",
    "                              n_shot=n_shot)\n",
    "\n",
    "# model set up\n",
    "cnn_lstm = Base_LSTM_CNN(emb_dim=768, seq_len=100, lstm_units=200, num_filters=30, kernel_sizes=[1, 3, 5], num_classes=2)\n",
    "prto_model = Protonet(cnn_lstm)\n",
    "\n",
    "train_proto(the_model=prto_model,\n",
    "            n_epochs=n_epochs,\n",
    "            learning_rate=lr,\n",
    "            train_loader_generator=train_sampler,\n",
    "            valid_loader_generator=valid_sampler,\n",
    "            n_eposides_train=32,\n",
    "            n_eposides_valid=3,\n",
    "            n_shot=n_shot,\n",
    "            estimate_theta_subsample_size=50,\n",
    "            the_device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Function Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 2.417734375000009\n",
      "Accuracy: 0.8, Theta: 2.500781250000009\n",
      "Accuracy: 0.8, Theta: 2.475625000000009\n",
      "Accuracy: 0.8, Theta: 2.226640625000009\n",
      "Accuracy: 0.8, Theta: 2.1929687500000083\n",
      "Accuracy: 0.8, Theta: 1.7365625000000064\n",
      "Accuracy: 0.8, Theta: 1.387656250000005\n",
      "Accuracy: 0.8, Theta: 1.7117187500000064\n",
      "Accuracy: 0.8, Theta: 1.9567968750000078\n",
      "Accuracy: 0.8, Theta: 2.2785156250000087\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 8 + [0] * 2)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 5.165078125000019\n",
      "Accuracy: 0.8, Theta: 5.78492187500002\n",
      "Accuracy: 0.8, Theta: 4.940312500000017\n",
      "Accuracy: 0.8, Theta: 4.443984375000015\n",
      "Accuracy: 0.8, Theta: 5.2804687500000185\n",
      "Accuracy: 0.8, Theta: 5.304843750000019\n",
      "Accuracy: 0.8, Theta: 4.715937500000017\n",
      "Accuracy: 0.8, Theta: 4.9064843750000176\n",
      "Accuracy: 0.8, Theta: 4.251250000000015\n",
      "Accuracy: 0.8, Theta: 5.44898437500002\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 80 + [0] * 20)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 8.504296875000028\n",
      "Accuracy: 0.8, Theta: 8.651328125000031\n",
      "Accuracy: 0.8, Theta: 8.76437500000003\n",
      "Accuracy: 0.8, Theta: 8.174921875000027\n",
      "Accuracy: 0.8, Theta: 8.22570312500003\n",
      "Accuracy: 0.8, Theta: 8.237812500000029\n",
      "Accuracy: 0.8, Theta: 9.249140625000033\n",
      "Accuracy: 0.8, Theta: 8.54468750000003\n",
      "Accuracy: 0.8, Theta: 8.44351562500003\n",
      "Accuracy: 0.8, Theta: 8.54250000000003\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 800 + [0] * 200)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8, Theta: 12.685703125000048\n",
      "Accuracy: 0.8, Theta: 13.119296875000044\n",
      "Accuracy: 0.8, Theta: 12.416953125000044\n",
      "Accuracy: 0.8, Theta: 12.268437500000044\n",
      "Accuracy: 0.8, Theta: 12.409062500000047\n",
      "Accuracy: 0.8, Theta: 11.957890625000042\n",
      "Accuracy: 0.8, Theta: 12.476484375000046\n",
      "Accuracy: 0.8, Theta: 12.214531250000043\n",
      "Accuracy: 0.8, Theta: 12.142500000000043\n",
      "Accuracy: 0.8, Theta: 12.302031250000045\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "for _ in range(10):\n",
    "    valid_diffs = np.random.exponential(2.0, sample_size) - 1.5\n",
    "    valid_label = np.ones(sample_size)\n",
    "    valid_pred = np.array([1] * 8000 + [0] * 2000)\n",
    "    np.random.shuffle(valid_pred)\n",
    "    cur_resp = np.where(valid_pred == valid_label, 1, 0)\n",
    "    temp_theta = scoring.calculate_theta(difficulties=valid_diffs, response_pattern=cur_resp)[0]\n",
    "    print(f\"Accuracy: {cur_resp.mean()}, Theta: {temp_theta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
